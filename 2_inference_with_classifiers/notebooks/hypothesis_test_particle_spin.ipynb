{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with classifiers\n",
    "\n",
    "# Hypothesis-testing: particle spin example in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for notebook in Google Colab (otherwise not needed)\n",
    "import os\n",
    "\n",
    "# Install dependencies\n",
    "!pip install numpy pandas matplotlib scikit-learn torch --quiet\n",
    "\n",
    "# ---- Download utils.py ----\n",
    "if not os.path.exists(\"utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/utils.py\n",
    "\n",
    "# ---- Create data directory ----\n",
    "os.makedirs(\"data_observed\", exist_ok=True)\n",
    "\n",
    "# ---- Base raw URL ----\n",
    "base_url = \"https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/data_observed\"\n",
    "\n",
    "files = [\n",
    "    \"data_spin_extension.csv\",\n",
    "    \"data_spin.csv\",\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f\"data_observed/{f}\"):\n",
    "        !wget -q {base_url}/{f} -O data_observed/{f}\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use what we have learned in the [previous notebook](hypothesis_test_simple.ipynb) and apply to a research problem when the analytic likelihood is not known. \n",
    "\n",
    "This example concerns the measurement of particle's spin; a purely quantum mechanical property of an elementary particle. It is an intrinsic form of angular momentum, but unlike everyday spinning objects, it does not correspond to any literal rotation in space. Instead, it influences how particles behave, decay, and interact with forces, making it possible to distinguish different types of particles by their spin values.\n",
    "\n",
    "We are going to study the spin of a hypothetical particle $X$, by performing an experiment to look at how it decays.\n",
    "\n",
    "### Aim: \n",
    "\n",
    "Infer the spin-configuration of a particle $X$. \n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Three observables ($x_1$, $x_2$, $x_3$) related to the decay products of $X \\rightarrow aa$. The observed dataset contains $N_{obs}=10$ particle $X$ decays, and is stored in the file `data_spin.csv`.\n",
    "\n",
    "### Hypotheses: \n",
    "* $\\mathcal{H}_0$ = Spin-0 particle\n",
    "* $\\mathcal{H}_1$ = Spin-1 particle\n",
    "\n",
    "### Simulation: \n",
    "\n",
    "We have a faithful particle decay simulator which can reproduce the observable distributions of $X \\rightarrow aa$ under each hypothesis. The simulator can generate $N$ decays using the following code:\n",
    "```python\n",
    "sim_H0 = run_simulation(N, hypothesis='H0')\n",
    "sim_H1 = run_simulation(N, hypothesis='H1')\n",
    "```\n",
    "\n",
    "### Tasks: \n",
    "1. Data exploration to understand the problem\n",
    "1. Train a binary classifier to distinguish decays generated under $\\mathcal{H}_0$ from those under $\\mathcal{H}_1$\n",
    "1. Use the classifier output to perform a hypothesis test on the observed data and infer the spin-configuration of particle $X$.\n",
    "\n",
    "\n",
    "## Data exploration\n",
    "\n",
    "We will begin by generating synthetic data (simulation) and looking at the expected observable distributions under the two hypotheses. Now the simulator is a \"black-box\"... all we know is that it produces faithful synthetic data under the two hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_simulation\n",
    "N_train = 100000\n",
    "sim_H0 = run_simulation(N_train, hypothesis='H0')\n",
    "sim_H1 = run_simulation(N_train, hypothesis='H1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot observable distributions of synthetic data for each hypothesis\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16,4)) \n",
    "axs[0].hist(sim_H0['x1'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "axs[0].hist(sim_H1['x1'], bins=50, range=(-4,4), alpha=0.5, label='Simulation:H1', density=True)\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].hist(sim_H0['x2'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "axs[1].hist(sim_H1['x2'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H1', density=True)\n",
    "axs[1].set_xlabel('x2')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "axs[2].hist(sim_H0['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='Simulation: H0', density=True)\n",
    "axs[2].hist(sim_H1['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='Simulation: H1', density=True)\n",
    "axs[2].set_xlabel('x3')\n",
    "axs[2].set_ylabel('Density')\n",
    "axs[2].legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $(x1,x2)$ appear to be Gaussian distributed, where $x_1$ differs in mean between $\\mathcal{H}_0$ and $\\mathcal{H}_1$, while $x_2$ differs in the width. The third observable $x_3$ follows a sinusoidal-like distribution for $\\mathcal{H}_1$, and is uniform for $\\mathcal{H}_0$.\n",
    "\n",
    "We can also look at 2D pairs of observables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "axs[0][0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0][0].set_xlabel('x1')\n",
    "axs[0][0].set_ylabel('x2')\n",
    "axs[0][0].set_title('Simulation: H0')\n",
    "axs[1][0].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1][0].set_xlabel('x1')\n",
    "axs[1][0].set_ylabel('x2')\n",
    "axs[1][0].set_title('Simulation: H1')\n",
    "axs[0][1].hist2d(sim_H0['x1'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][1].set_xlabel('x1')\n",
    "axs[0][1].set_ylabel('x3')\n",
    "axs[0][1].set_title('Simulation: H0')\n",
    "axs[1][1].hist2d(sim_H1['x1'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][1].set_xlabel('x1')\n",
    "axs[1][1].set_ylabel('x3')\n",
    "axs[1][1].set_title('Simulation: H1')\n",
    "axs[0][2].hist2d(sim_H0['x2'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][2].set_xlabel('x2')\n",
    "axs[0][2].set_ylabel('x3')\n",
    "axs[0][2].set_title('Simulation: H0')\n",
    "axs[1][2].hist2d(sim_H1['x2'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][2].set_xlabel('x2')\n",
    "axs[1][2].set_ylabel('x3')\n",
    "axs[1][2].set_title('Simulation: H1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two observables $x_1$ and $x_1$ have a different correlation structure for the two hypotheses:\n",
    "* $\\mathcal{H}_0$ (spin 0) = positive correlation\n",
    "* $\\mathcal{H}_1$ (spin 1) = uncorrelated\n",
    "\n",
    "The classifier will be able to leverage this higher-dimensional information (correlation between features) to better discriminate between $\\mathcal{H}_0$ and $\\mathcal{H}_1$. Better discrimination leads to an improvement in the statistical power of the test.\n",
    "\n",
    "Let's look at how the observed dataset is distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_obs = pd.read_csv('data_observed/data_spin.csv')\n",
    "N_obs = len(data_obs)\n",
    "data_obs.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16,4))\n",
    "ax[0].hist(data_obs['x1'], bins=50, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[0].hist(sim_H0['x1'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax[0].hist(sim_H1['x1'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax[0].set_xlabel('x1')\n",
    "ax[0].set_ylabel('Density')\n",
    "ax[0].legend(loc='upper left')\n",
    "\n",
    "ax[1].hist(data_obs['x2'], bins=50, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[1].hist(sim_H0['x2'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax[1].hist(sim_H1['x2'], bins=50, range=(-4,4), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax[1].set_xlabel('x2')\n",
    "ax[1].set_ylabel('Density')\n",
    "ax[1].legend(loc='upper left')\n",
    "\n",
    "ax[2].hist(data_obs['x3'], bins=50, range=(-np.pi,np.pi), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[2].hist(sim_H0['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax[2].hist(sim_H1['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax[2].set_xlabel('x3')\n",
    "ax[2].set_ylabel('Density')\n",
    "ax[2].legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also look at the 2D distributions\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "axs[0][0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0][0].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[0][0].set_xlabel('x1')\n",
    "axs[0][0].set_ylabel('x2')\n",
    "axs[0][0].set_title('Simulation: H0')\n",
    "axs[0][0].legend(loc='upper right')\n",
    "axs[1][0].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1][0].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[1][0].set_xlabel('x1')\n",
    "axs[1][0].set_ylabel('x2')\n",
    "axs[1][0].set_title('Simulation: H1')\n",
    "axs[1][0].legend(loc='upper right')\n",
    "axs[0][1].hist2d(sim_H0['x1'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][1].scatter(data_obs['x1'], data_obs['x3'], color='black', label='Observed Data')\n",
    "axs[0][1].set_xlabel('x1')\n",
    "axs[0][1].set_ylabel('x3')\n",
    "axs[0][1].set_title('Simulation: H0')\n",
    "axs[0][1].legend(loc='upper left')\n",
    "axs[1][1].hist2d(sim_H1['x1'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][1].scatter(data_obs['x1'], data_obs['x3'], color='black', label='Observed Data')\n",
    "axs[1][1].set_xlabel('x1')\n",
    "axs[1][1].set_ylabel('x3')\n",
    "axs[1][1].set_title('Simulation: H1')\n",
    "axs[1][1].legend(loc='upper left')\n",
    "axs[0][2].hist2d(sim_H0['x2'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][2].scatter(data_obs['x2'], data_obs['x3'], color='black', label='Observed Data')\n",
    "axs[0][2].set_xlabel('x2')\n",
    "axs[0][2].set_ylabel('x3')\n",
    "axs[0][2].set_title('H0')\n",
    "axs[0][2].legend(loc='upper left')\n",
    "axs[1][2].hist2d(sim_H1['x2'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][2].scatter(data_obs['x2'], data_obs['x3'], color='black', label='Observed Data')\n",
    "axs[1][2].set_xlabel('x2')\n",
    "axs[1][2].set_ylabel('x3')\n",
    "axs[1][2].set_title('Simulation: H1')\n",
    "axs[1][2].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell by eye which class the observed data belong to? Now it is not so simple!\n",
    "\n",
    "Let's train a classifier and set up a proper hypothesis test as in [Section 3](#simple_hypothesis_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n",
    "In the 1D Gaussian example notebook, we used a simple logistic regression for the classification task. This may not be sufficient when we extend to more complicated inference tasks. \n",
    "\n",
    "In this 3D example, we will now use a neural network to classify $\\mathcal{H}_0$ vs $\\mathcal{H}_1$. This will be a simple multi-layer perceptron (MLP), which will be trained using the simulated datasets. We will use the `torch` library to build the MLP.\n",
    "\n",
    "It is important to note that this approach to SBI does not depend on the architecture of the classifier. You can use whichever ML model is most appropriate to the problem (e.g. BDT, CNN, GNN etc). All the method requires is training with a BCE loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas dataframe to a torch tensor\n",
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training dataset \n",
    "sim_H0['label'] = 0\n",
    "sim_H1['label'] = 1\n",
    "sim = pd.concat([sim_H0, sim_H1], ignore_index=True)\n",
    "\n",
    "# Split into test/train sets\n",
    "test_size = 0.2\n",
    "sim_train, sim_test = train_test_split(sim, test_size=test_size, shuffle=True)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = df_to_tensor(sim_train[['x1', 'x2', 'x3']])\n",
    "y_train = df_to_tensor(sim_train[['label']])\n",
    "X_test = df_to_tensor(sim_test[['x1', 'x2', 'x3']])\n",
    "y_test = df_to_tensor(sim_test[['label']])\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 2048\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple feedforward MLP with two hidden layers: input -> hidden -> hidden -> output (1)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "input_size = 3\n",
    "hidden_size = 16\n",
    "model = SimpleMLP(input_size, hidden_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: track loss per epoch for training and testing datasets\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * inputs.size(0)\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the training history to see if we have reached the plateau..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable!\n",
    "\n",
    "Now let's take a look at the classifier output. We will plot the distribution of $\\hat{f}(x)$ for $\\mathcal{H}_0$ and $\\mathcal{H}_1$ from the test dataset, and compare this to the classifier output distribution for the observed data. \n",
    "\n",
    "Does this help indicate which hypothesis the data belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test simulation samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sim_test_outputs = model(X_test).numpy().flatten()\n",
    "sim_test_labels = y_test.numpy().flatten()\n",
    "\n",
    "# Also evaluate for the observed data\n",
    "X_obs = df_to_tensor(data_obs[['x1', 'x2', 'x3']])\n",
    "with torch.no_grad():\n",
    "    obs_outputs = model(X_obs).numpy().flatten()\n",
    "\n",
    "# Plot histograms of the classifier scores\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.hist(sim_test_outputs[sim_test_labels==0], bins=100, range=(0,1), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax.hist(sim_test_outputs[sim_test_labels==1], bins=100, range=(0,1), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax.hist(obs_outputs, bins=100, range=(0,1), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.set_xlabel('Classifier output, $\\hat{f}(x)$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two data points towards $\\hat{f}(x) \\approx 1$, which are very inconsistent with $\\mathcal{H}_0$ (spin-0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing\n",
    "Following the same procedure as in [Section 3](#simple_hypothesis_test), we will now use the classifier output to perform a hypothesis test. \n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "In exactly the same way, we can convert the classifier output to the density ratio using the likelihood-ratio trick:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)}\n",
    "$$\n",
    "\n",
    "And with this, we can approximate the test-statistic over the full ($N_{obs}=10$) dataset:\n",
    "\n",
    "$$\n",
    "t = -2 \\ln{\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}} \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's write python functions to be compatible with the torch-based model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_torch(X, model, clip=1e-10):\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.tensor(X, dtype=torch.float32)).numpy().flatten()\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr\n",
    "\n",
    "def test_statistic_torch(X, model):\n",
    "    llr = log_likelihood_ratio_torch(X, model)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the hypothesis test we first need to generate pseudo-experiments under each hypothesis with $N_{obs}=10$. We will build up distributions of the learned test-statistic for each hypothesis. We then calculate the test-statistic value for the observed dataset, and determine the $p$-value with respect to the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0_clf = []\n",
    "test_statistic_H1_clf = []\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = run_simulation(N_obs, hypothesis='H0')\n",
    "    t_H0_clf = test_statistic_torch(samples_H0[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H0_clf.append(t_H0_clf)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = run_simulation(N_obs, hypothesis='H1')\n",
    "    t_H1_clf = test_statistic_torch(samples_H1[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H1_clf.append(t_H1_clf)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs_clf = test_statistic_torch(data_obs[['x1', 'x2', 'x3']].values, model)\n",
    "\n",
    "# Calculate the p-value for H0\n",
    "p_value_H0_clf = np.sum(np.array(test_statistic_H0_clf) <= t_obs_clf) / N_toys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we will calculate the Type-I and Type-II errors, as well as the power of the statistical test for fixed $\\alpha=0.05$. We will also compute the errors for different $\\alpha$ values to construct the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the type-1 and type-2 errors for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "power_clf = 1 - type_2_error_clf\n",
    "\n",
    "# Plot the distributions of the test statistic under both hypotheses, as well as the observed case\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "axs[0].hist(test_statistic_H0_clf, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H0')\n",
    "axs[0].hist(test_statistic_H1_clf, bins=50, alpha=0.5, range=(-50,50), label='Simulation:H1')\n",
    "axs[0].axvline(t_obs_clf, color='black', linestyle='solid', linewidth=2, label='Observed data ($p$={:.3f})'.format(p_value_H0_clf))\n",
    "axs[0].axvline(critical_value_clf, color='red', linestyle='dashed', linewidth=2, label='Critical value (α={})'.format(alpha))\n",
    "axs[0].set_xlabel('$t = -2 \\\\sum{\\\\ln{\\\\frac{\\\\hat{f}}{1-\\\\hat{f}}}}$')\n",
    "axs[0].set_xlim(-50, 50)\n",
    "axs[0].set_ylabel('Number of pseudo-experiments')\n",
    "axs[0].legend(loc='upper right')\n",
    "# Add text to plot with type-1, type-2 errors and power\n",
    "textstr = '\\n'.join((\n",
    "    'Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    'Power (1-$\\\\beta$): {:.3f}'.format(power_clf)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "axs[0].text(0.05, 0.95, textstr, transform=axs[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Plot ROC curve\n",
    "alpha_vals = np.linspace(0.0, 1.0, 1000)\n",
    "beta_vals_clf = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "    type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "    beta_vals_clf.append(type_2_error_clf)\n",
    "fpr = alpha_vals\n",
    "tpr_clf = 1 - np.array(beta_vals_clf)\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc_clf = np.trapezoid(tpr_clf, fpr)\n",
    "axs[1].plot(fpr, tpr_clf, label='Learned test-statistic (AUC = {:.3f})'.format(auc_clf), color='orange')\n",
    "axs[1].plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "axs[1].set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "axs[1].set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "axs[1].legend(loc='best')\n",
    "axs[1].set_xlim(0,1)\n",
    "axs[1].set_ylim(0,1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We were presented with a research problem with an unknown (intractable) likelihood i.e. we do not know the probability of observing data sample $(x_1,x_2,x_3)$ under hypothesis $\\mathcal{H}_i$: $p(x_1,x_2,x_3|\\mathcal{H}_i)$.\n",
    "\n",
    "Nevertheless, we had access to an accurate simulation of the data. We used this simulation to train a classifier to distinguish $\\mathcal{H}_0$ (spin-0 particle) from $\\mathcal{H}_1$ (spin-1 particle).\n",
    "\n",
    "The output of the classifier was used to approximate the probability density ratio. This was then used to calculate the log-likelihood-ratio test-statistic over the full dataset. Using a frequentist approach, we compared the observed value of the \"learned\" test-statistic to the distributions generated from pseudo-experiments under each hypothesis. \n",
    "\n",
    "The data is inconsistent with the null hypothesis ($\\mathcal{H}_0$) with a $p$-value of less than 0.05. Therefore we can reject $\\mathcal{H}_0$ and infer that particle $X$ is spin-one!\n",
    "\n",
    "This is an end-to-end example of using ML to perform SBI. In the next section we will go beyond hypothesis testing, and show how to use ML classifiers for parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Extensions\n",
    "\n",
    "### Amortized inference\n",
    "One great advantage of this approach is that we can use the trained classifier to perform inference on different observations, assuming that the experiment is performed under identical conditions. This means we do not need to re-run an expensive inference procedure each time. In other words, we have **amortized** the inference for future experiments. This becomes extremely useful when dealing with problems with extremely complex likelihoods.\n",
    "\n",
    "We re-run the experiment, where $X \\rightarrow aa$ decays are recorded over a longer period such that we obtain a larger dataset. This dataset can be found in `data_spin_extension.csv`. Your task is to repeat the hypothesis-test procedure using the larger dataset with the trained classifier. How do your conclusions change? Is this in line with what you expect from taking more data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extended data\n",
    "data_obs_extended = pd.read_csv('data_observed/data_spin_extension.csv')\n",
    "N_obs = len(data_obs_extended)\n",
    "data_obs_extended.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pseudo-experiments with N=50 and build up distributions of the test statistic under both hypotheses\n",
    "N_toys = 10000\n",
    "test_statistic_H0_clf = []\n",
    "test_statistic_H1_clf = []\n",
    "\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = run_simulation(N_obs, hypothesis='H0')\n",
    "    t_H0_clf = test_statistic_torch(samples_H0[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H0_clf.append(t_H0_clf)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = run_simulation(N_obs, hypothesis='H1')\n",
    "    t_H1_clf = test_statistic_torch(samples_H1[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H1_clf.append(t_H1_clf)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs_clf = test_statistic_torch(data_obs[['x1', 'x2', 'x3']].values, model)\n",
    "\n",
    "# Calculate the p-value for H0\n",
    "p_value_H0_clf = np.sum(np.array(test_statistic_H0_clf) <= t_obs_clf) / N_toys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the type-1 and type-2 errors for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "power_clf = 1 - type_2_error_clf\n",
    "\n",
    "# Plot the test statistic distributions and compare to the observed value\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.hist(test_statistic_H0_clf, bins=50, alpha=0.5, range=(-100,100), label='Simulation: H0')\n",
    "ax.hist(test_statistic_H1_clf, bins=50, alpha=0.5, range=(-100,100), label='Simulation: H1')\n",
    "ax.axvline(t_obs_clf, color='black', linestyle='solid', linewidth=2, label='Observed data ($p$={:.3f})'.format(p_value_H0_clf))\n",
    "ax.axvline(critical_value_clf, color='red', linestyle='dashed', linewidth=2, label='Critical value (α={})'.format(alpha))\n",
    "ax.set_xlabel('$t = -2 \\\\sum{\\\\ln{\\\\frac{\\\\hat{f}}{1-\\\\hat{f}}}}$')\n",
    "ax.set_xlim(-100, 100)\n",
    "ax.set_ylabel('Number of pseudo-experiments')\n",
    "ax.legend(loc='upper right')\n",
    "# Add text to plot with type-1, type-2 errors and power\n",
    "textstr = '\\n'.join((\n",
    "    'Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    'Power (1-$\\\\beta$): {:.3f}'.format(power_clf)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the overlap of the test-statistic distributions with the additional data?\n",
    "\n",
    "We can conclude with extremely high confidence that the data is inconsistent with the null hypothesis (Spin 0), and that particle $X$ is spin-one!\n",
    "\n",
    "And **importantly**, there was no need to re-train the classifier or re-run the expensive inference procedure. We simply applied the same trained classifier to the new dataset, and re-evaluated the test-statistic and $p$-value. This is a key advantage of this approach to SBI, and is what we mean by amortized inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
