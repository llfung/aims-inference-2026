{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with classifiers\n",
    "\n",
    "# Hypothesis-testing: simple 1D Gaussian\n",
    "\n",
    "# Index:  <a id='index'></a>\n",
    "1. [Introduction](#intro)\n",
    "1. [Learning the likelihood-density ratio with classifiers](#sbi_with_classifiers)\n",
    "1. [Simple hypothesis test with 1D Gaussian example](#simple_hypothesis_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for notebook in Google Colab (otherwise not needed)\n",
    "import os\n",
    "\n",
    "# Install dependencies\n",
    "!pip install numpy pandas matplotlib scikit-learn torch --quiet\n",
    "\n",
    "# ---- Download utils.py ----\n",
    "if not os.path.exists(\"utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/utils.py\n",
    "\n",
    "# ---- Create data directory ----\n",
    "os.makedirs(\"data_observed\", exist_ok=True)\n",
    "\n",
    "# ---- Base raw URL ----\n",
    "base_url = \"https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/data_observed\"\n",
    "\n",
    "files = [\n",
    "    \"data_1d_gauss.csv\",\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f\"data_observed/{f}\"):\n",
    "        !wget -q {base_url}/{f} -O data_observed/{f}\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 1. Introduction <a id='intro'></a>\n",
    "\n",
    "In this notebook you will learn how to use a simple Machine Learning (ML) classifier for simulation-based inference (SBI). Let's begin by introducing what this means...\n",
    "\n",
    "Statistical inference is a key component of the scientific method. It explains how we extract any understanding from the data we collect. As you have learnt from the lecture, the **likelihood function** holds the key to inference. \n",
    "\n",
    "Let's say we perform an experiment $N_{obs}$ times and observe the data set $\\mathcal{D} = \\{x_i\\}^{N_{obs}}_{i=1}$, where $x \\in \\mathbb{R}^d$ is the vector of features describing the data of dimension $d$. We have a theoretical model defined by parameters $\\theta$, which describes the data. In order to infer the values of $\\theta$ from the data, we need the (conditional) probability density: $p(x|\\theta)$. You can read this as \"the probability to observe data $x$ given parameters $\\theta$\".\n",
    "\n",
    "The **likelihood** is then constructed as the product of the densities over the observed data:\n",
    "\n",
    "$$ \n",
    "p(\\mathcal{D}|\\theta) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} p(x_i|\\theta) \n",
    "$$\n",
    "\n",
    "This tells us the probability of observing dataset $\\mathcal{D}$ given $\\theta$. There are then two paradigms for inference:\n",
    "\n",
    "* <u>**Frequentist inference**</u>: the interpretation here is over hypothetical repetitions of the experiment i.e. probability is the fraction of experiments under which the true value of $\\theta$ would be included in the estimator’s confidence interval. For this, we construct a test-statistic $t$, and compare the observed value of the test-statistic to distributions under different values of $\\theta$. The Neyman-Pearson lemma tells us that (for a simple hypothesis test) the most powerful test-statistic is the (log)-likelihood ratio:\n",
    "\n",
    "    $$\n",
    "    t = -2\\ln{\\frac{p(\\mathcal{D}|\\theta)}{p(\\mathcal{D}|\\theta_0)}}\n",
    "    $$\n",
    "\n",
    "    where $\\theta_0$ is a reference value of the parameter vector $\\theta$. The \"log\" is used to avoid very small numbers (product over probilities $\\rightarrow$ sum over log(probabilities)). The factor of -2 is used to align the test-statistic with the $\\chi^2$ distribution (see Wilks' theorem discussion later).\n",
    "\n",
    "* <u>**Bayesian inference**</u>: the interpretation here is over degrees of belief about parameter values. We use Bayes Theorem to estimate the posterior density:\n",
    "\n",
    "    $$\n",
    "    p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p(\\mathcal{D})}\n",
    "    $$\n",
    "\n",
    "    from which we build credible intervals for parameters as a function of the data.\n",
    "\n",
    "Both paradigms rely on the likelihood: $p(\\mathcal{D}|\\theta)$. These notebooks have a frequentist flavour (put together by a particle physicist), but there is nothing stopping you applying these ideas in a Bayesian formalism.\n",
    "\n",
    "## Intractable likelihoods\n",
    "\n",
    "For many scientific domains, we are no longer in the regime of simple models with analytic (tractable) likelihoods. The systems that we are trying to model become increasingly complex with many unobserved (latent) variables, $z$. Take disease modelling over a population:\n",
    "\n",
    "![disease_modelling](disease_modelling.png)\n",
    "\n",
    "We need to model the effect of social mixing, mask wearing, level of caution etc (latent variables) in order to infer something meaningful about the data. \n",
    "\n",
    "This crucially cannot be done analytically. The likelihood becomes a complicated integral over all possible trajectories through latent space:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\theta) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\int dz \\,p(x_i|\\theta,z) \n",
    "$$\n",
    "\n",
    "How can we approximate this intractable likelihood? Answer: **Simulation**.\n",
    "\n",
    "We can use (stochastic) simulators to generate synthetic data under different model parameters $\\theta$. These simulators are typically very complex in order to encapsulate all the underlying physics. **Simulation-based inference** (SBI) refers to the comparison of the synthetic data to the observed data, in order to infer something about $\\theta$.\n",
    "\n",
    "## Simulation-based inference\n",
    "\n",
    "There are two traditional approaches to SBI:\n",
    "\n",
    "* Approximate Bayesian Computation (ABC): which you have already covered.\n",
    "\n",
    "* Probability density estimation using the simulated (synthetic) data via histogramming or kernel-density estimation.\n",
    "\n",
    "Over recent years, scientists across many disciplines have turned to Machine Learning (ML) for SBI. The recent boom has lead to the development of a plethora of ML techniques, most of which cannot be covered in this tutorial. Instead, we will lay the foundations by showing how a simple ML classifer can be used for hypothesis testing, and later parameter estimation.\n",
    "\n",
    "This should help you bridge the gap between ML and some core concepts in statistics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning the likelihood ratio with classifiers <a id='sbi_with_classifiers'></a>\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Binary classifiers are tasked with discriminating between two hypotheses: $\\mathcal{H}_0$ and $\\mathcal{H}_1$. They are trained by minimising the binary cross-entropy (BCE) loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}[f] = -\\frac{1}{N} \\sum^{N}_{i=1} y_i \\ln{f(x_i)} + (1-y_i)\\ln{(1-f(x_i))}\n",
    "$$\n",
    "\n",
    "where the sum is over $N$ samples $x_i$ drawn from the (conditional) probability densities $p(x|\\mathcal{H}_0)$ or $p(x|\\mathcal{H}_1)$ with assigned labels $y_i=0$ or $y_i=1$, respectively. Note, for SBI these samples are produced with the simulator.\n",
    "\n",
    "The quantity $f(x_i)$ is the classifier decision function (i.e. the output of the classifier). The optimal decision function (in the infinite sample limit i.e. as $N \\rightarrow \\infty$) which minimises the (BCE) loss function is:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)+p(x_i|\\mathcal{H}_1)}\n",
    "$$\n",
    "\n",
    "assuming that the two hypotheses are equally represented in the training data. \n",
    "\n",
    "It is important to note that our trained classifier $\\hat{f}(x_i)$ will be an estimator of the optimal decision function: $\\hat{f}(x_i) \\approx f(x_i)$. For a classification task, this will just lead to a mis-labelling of the data. However, for inference, it can lead to a bias in our measurement. Hence, we take great care in validating our SBI ML algorithms when applied in real-world scenarios.\n",
    "\n",
    "With a simple re-arranging known as the likelihood-ratio trick [1], we arrive at an approximation of the (conditional) probability density ratio between the two hypotheses for a single sample $x_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H_1})}{p(x_i|\\mathcal{H_0})}.\n",
    "$$\n",
    "\n",
    "The likelihood ratio (which is what we want for Frequentist inference) is then constructed by taking the product of the probability density ratios for all samples in the observed data set. \n",
    "\n",
    "$$\n",
    "\\frac{p(\\mathcal{D}|\\mathcal{H}_0)}{p(\\mathcal{D}|\\mathcal{H}_1)} = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\frac{p(x_i|\\mathcal{H_1})}{p(x_i|\\mathcal{H_0})} \\approx \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\n",
    "$$\n",
    "\n",
    "Under the Neyman-Pearson lemma, the (twice negative) log-likelihood ratio provides the most powerful test-statistic for a simple hypothesis test:\n",
    "\n",
    "$$\n",
    "t = -2 \\ln{\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}} \\approx -2 \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "We can use this \"learned\" test-statistic to perform a hypothesis test!\n",
    "\n",
    "</div>\n",
    "\n",
    "In summary, we first showed how the output of a classifier can be used to approximate the probability density ratio between two hypotheses for a single sample, $x_i$. This can then be used to define a test-statistic over a full dataset. We will now show how to use the test-statistic for inference by performing a hypothesis test. This notebook shows a very simple example where the analytic likelihood is known and used to compare to the \"learned\" test-statistic.\n",
    "\n",
    "[1] - K. Cranmer, J. Pavez and G. Louppe, *Approximating Likelihood Ratios with Calibrated Discriminative Classifiers* (2016). [arXiv:1506.02169](https://arxiv.org/abs/1506.02169)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple hypothesis test  <a id='simple_hypothesis_test'></a>\n",
    "\n",
    "Let's show how to use a binary classifier to perform a two-class hypothesis test for a simple problem. You will use the hypothesis testing procedure that we discussed during the lecture. The problem is as follows:\n",
    "\n",
    "* There are two possible hypotheses (classes) to explain the data. Data produced from the classes have one observable $x$ which is distributed according to a 1D Gaussian with unit width. Class 0 has a mean of zero, and class 1 has a mean of one. We will define class 0 as the null hypothesis ($\\mathcal{H}_0$), and class 1 as the alternative hypothesis ($\\mathcal{H}_1$).\n",
    "* An experiment is performed in which $x$ is measured for $N_{obs}=10$ independent samples. The observed data are provided in the file `data_observed/data_1d_gauss.csv`.\n",
    "* We are going to set up a hypothesis test to infer which class the data belong to. We will first demonstrate how to do this using the analytic likelihood, where we will use the log-likelihood ratio as a test-statistic. \n",
    "* Following this we will show how to learn the log-likelihood ratio from simulated samples (synthetic data) using machine learning. \n",
    "* We will then compare the performance to the analytic approach.\n",
    "\n",
    "Obviously, SBI is overkill for this simple problem where the analytic likelihood is known. We will show in the next notebook how one can extend this to a more complex problem (with an unknown likelihood) to perform inference on the data.\n",
    "\n",
    "Let's begin by importing the relevant libraries and defining the function used to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 1D gaussian synthetic data (x) for mean=loc and stdev=scale\n",
    "def run_simulation(N, loc, scale=1):\n",
    "    data = np.random.normal(loc=loc, scale=scale, size=N)\n",
    "    return pd.DataFrame(data, columns=['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate synthetic data from each of the two hypotheses. These are subsequently combined into a single dataframe which will later use to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_class = 100000\n",
    "# Generate synthetic training data for two classes\n",
    "sim_H0 = run_simulation(num_per_class, loc=0, scale=1)\n",
    "sim_H0['label'] = 0\n",
    "sim_H1 = run_simulation(num_per_class, loc=1, scale=1)\n",
    "sim_H1['label'] = 1\n",
    "sim = pd.concat([sim_H0, sim_H1]).reset_index(drop=True)\n",
    "\n",
    "# Plot histograms of the synthetic training data\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(sim_H0['x'], bins=100, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax.hist(sim_H1['x'], bins=100, range=(-4,4), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data is stored in `data_observed/data_1d_gauss.csv`. Let's plot the observed data on top of the expected distributions for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv file\n",
    "data_obs = pd.read_csv('data_observed/data_1d_gauss.csv')\n",
    "N_obs = len(data_obs)\n",
    "\n",
    "# Plot histogram of the observed data\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(data_obs['x'], bins=100, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.hist(sim_H0['x'], bins=100, range=(-4,4), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax.hist(sim_H1['x'], bins=100, range=(-4,4), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell by eye which class the observed data belong to? In any case, let's perform a proper hypothesis test. We will start with the analytic solution.\n",
    "\n",
    "## Analytic likelihood-ratio\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The probability of observation $x_i$ is given by the following formula, assuming $x$ follows a Gaussian distribution with mean $\\mu$ and width $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "p(x_i|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Therefore, the likelihood over dataset $\\mathcal{D} = \\{x_i\\}$ of $N_{obs}$ samples is:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mu,\\sigma) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} p(x_i|\\mu,\\sigma)\n",
    "$$\n",
    "\n",
    "As discussed above, the Neyman-Pearson lemma tells us that the (log)-likelihood ratio is the most powerful test-statistic for simple hypothesis testing ($\\mathcal{H}_0$ vs $\\mathcal{H}_1$). We will use: \n",
    "\n",
    "$$\n",
    "t =  -2 \\ln {\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Do you know why we use the \"twice-negative\" as convention? The answer lies in **Wilk's theorem**: the twice-negative log-likelihood ratio asymptotically approaches the $\\chi^2$ distribution under the null hypothesis. This means we can use the properties of the $\\chi^2$ function to determine the probability distribution of the test-statistic, and subsequently the coverage/confidence intervals. Anyway we will come to this in the parameter estimation section.\n",
    "\n",
    "Let's code this up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate log-likelihood ratio between two hypotheses with (mu0, sigma0) vs (mu1, sigma1)\n",
    "def log_likelihood_ratio(x, mu0=0, mu1=1, sigma0=1, sigma1=1):\n",
    "    ll_H0 = -0.5 * ((x - mu0) / sigma0)**2 - np.log(sigma0) - 0.5 * np.log(2 * np.pi)\n",
    "    ll_H1 = -0.5 * ((x - mu1) / sigma1)**2 - np.log(sigma1) - 0.5 * np.log(2 * np.pi)\n",
    "    return ll_H1 - ll_H0\n",
    "\n",
    "# Function to calculate test statistic over all events \n",
    "def test_statistic(x, mu0=0, mu1=1, sigma0=1, sigma1=1):\n",
    "    llr = log_likelihood_ratio(x, mu0, mu1, sigma0, sigma1)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a hypothesis test we need to first generate synthetic datasets (\"pseudo-experiments\") which are representative of the observed data ($N_{obs}=10$), under each hypothesis. For each pseudo-experiment we will calculate the value of the test statistic, building up a distributions of the test statistic under the two hypotheses.\n",
    "\n",
    "We then calculate the test-statistic value for the observed data, and compare this to the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0 = []\n",
    "test_statistic_H1 = []\n",
    "\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = run_simulation(N_obs, loc=0)\n",
    "    t_H0 = test_statistic(samples_H0['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H0.append(t_H0)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = run_simulation(N_obs, loc=1)\n",
    "    t_H1 = test_statistic(samples_H1['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H1.append(t_H1)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs = test_statistic(data_obs['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "\n",
    "# Plot the distributions of the test statistic under both hypotheses, as well as the observed case\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(test_statistic_H0, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H0')\n",
    "ax.hist(test_statistic_H1, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H1')\n",
    "ax.axvline(t_obs, color='black', linestyle='solid', linewidth=2, label='Observed data')\n",
    "ax.set_xlabel('t')\n",
    "ax.set_xlim(-50, 65)\n",
    "ax.set_ylabel('Number of pseudo-experiments')\n",
    "ax.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the test-statistic distributions we can calculate the $p$-value with respect to the null hypothesis ($\\mathcal{H}_0$) by integrating the left-hand tail of the $\\mathcal{H}_0$ distribution up to the observed value of the test-statistic. This $p$-value tells us:\n",
    "* Assuming $\\mathcal{H}_0$ is true, what is the probability of obtaining data at least as extreme as what you observed, over a run of repeated identical experiments.\n",
    "\n",
    "If the $p$-value is below some pre-defined critical threshold ($\\alpha=0.05$), then we can reject the null-hypothesis at the $1-\\alpha$ confidence level (CL). \n",
    "\n",
    "We can calculate the Type-1 and Type-2 error rate for fixed $\\alpha$ and subsequently the power of the test. The error rates are defined as follows:\n",
    "* Type-1: assuming $\\mathcal{H}_0$ is true, it is the probability of incorrectly rejecting $\\mathcal{H}_0$ ($=\\alpha$) i.e. false positive rate.\n",
    "* Type-2: assuming $\\mathcal{H}_1$ is true, it is the probability of incorrectly failing to reject $\\mathcal{H}_0$ ($\\beta$) i.e. it quantifies the fraction of the $\\mathcal{H}_1$ distribution which lies beyond the critical value. In this way ($1-\\beta$) is equal to the true positive rate (which is also referred to as the statistical power). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-value for H0\n",
    "p_value_H0 = np.sum(np.array(test_statistic_H0) <= t_obs) / N_toys\n",
    "\n",
    "# Calculate the type-1 and type-2 errors and the statistical power for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value = np.percentile(test_statistic_H0, alpha * 100)\n",
    "type_2_error = np.sum(np.array(test_statistic_H1) > critical_value) / N_toys\n",
    "power = 1 - type_2_error    \n",
    "\n",
    "# Plot results again with additional annotations\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.hist(test_statistic_H0, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H0')\n",
    "ax.hist(test_statistic_H1, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H1')\n",
    "ax.axvline(t_obs, color='black', linestyle='solid', linewidth=2, label='Observed data: $p$={:.3f}'.format(p_value_H0))\n",
    "ax.axvline(critical_value, color='red', linestyle='dashed', linewidth=2, label='Critical value (α={})'.format(alpha))\n",
    "ax.set_xlabel('t')\n",
    "ax.set_xlim(-50, 65)\n",
    "ax.set_ylabel('Number of pseudo-experiments')\n",
    "ax.legend(loc='best', fontsize=8)\n",
    "# Add text to plot with type-1, type-2 errors and power\n",
    "textstr = '\\n'.join((\n",
    "    'Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    'Power (1-$\\\\beta$): {:.3f}'.format(power)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=6,\n",
    "       verticalalignment='top', bbox=props)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $p$-value is less than the critical value ($\\alpha=0.05$). We can reject the null hypothesis $\\mathcal{H}_0$ for this data at the 95% CL.\n",
    "\n",
    "We can also calculate a compatibility $p$-value with respect to the alternative hypothesis $\\mathcal{H}_1$. What we want to know is \"how extreme is the observed data compared to what I would expect under $\\mathcal{H}_1$? Since large values of the test-statistic look more like $\\mathcal{H}_0$, then here it is flipped i.e. is it the upper tail of the $\\mathcal{H}_1$ distribution which indicates the incompatibility with $\\mathcal{H}_1$. \n",
    "\n",
    "Let's calculate this quantity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value for alternative hypothesis\n",
    "p_value_H1 = np.sum(np.array(test_statistic_H1) >= t_obs) / N_toys\n",
    "\n",
    "# Print p-values for both hypotheses\n",
    "print('P-value for H0: {:.5f}'.format(p_value_H0))\n",
    "print('P-value for H1: {:.5f}'.format(p_value_H1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is much more compatible with the alternative hypothesis! Note this might not always be the case i.e. you may have two hypotheses, neither of which fit the data well. Under this scenario one should look towards forming a different hypothesis.\n",
    "\n",
    "A ROC curve tells us the trade-off between the Type-I error rate (false positives) and the 1 - Type-II error rate (true positives). We move the threshold ($\\alpha$) and calculate $1-\\beta$, as shown in the code block below. The area-under-curve (AUC) metric is a measure of how well the test statistic separates the $\\mathcal{H}_0$ and $\\mathcal{H}_1$ distributions i.e. the power of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = np.linspace(0.0, 1.0, 50)\n",
    "beta_vals = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value = np.percentile(test_statistic_H0, alpha * 100)\n",
    "    type_2_error = np.sum(np.array(test_statistic_H1) > critical_value) / N_toys\n",
    "    beta_vals.append(type_2_error)\n",
    "\n",
    "fpr = alpha_vals\n",
    "tpr = 1 - np.array(beta_vals)\n",
    "\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc = np.trapezoid(tpr, fpr)\n",
    "\n",
    "# Plot the ROC curve (beta vs alpha)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr, label='Analytic test-statistic (AUC = {:.3f})'.format(auc))\n",
    "ax.plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "ax.set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "ax.set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the likelihood-ratio\n",
    "\n",
    "As discussed in [Section 2](#sbi_with_classifiers) we can use a binary classifier to approximate the likelihood ratio. \n",
    "\n",
    "In this simple 1D example we will use <u>**logistic regression**</u>. Logistic regression is a simple linear classifer: it passes a linear (weighted) sum of the obvservable(s) through a sigmoid function, to map any number to a probability that the sample belongs to $\\mathcal{H}_0$ or $\\mathcal{H}_1$. By minimising the loss in training, we find the optimal weights for each observable. More information can be found in [1]. \n",
    "\n",
    "Note, the SBI approach extends to more complicated classifier architectures (e.g. BDTs, neural networks), which become useful when dealing with more complex inference problems i.e. higher dimensions, more complicated features.\n",
    "\n",
    "[1] - Hastie, T. et al, \"The Elements of Statistical Learning\", Section 4.4 [Link](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier to distinguish between H0 and H1\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "X_train = sim[['x']].values\n",
    "y_train = sim['label'].values.astype(int)\n",
    "# Fit classifier with BCE loss\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the classifier $\\hat{f}(x)$ has been trained, we can examine the output. Let's first generate independent simulation samples under the two hypotheses. We can then compare the classifier output score distribution from the simulation, to that of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier scores on independent test data from the two classes\n",
    "num_test_per_class = 100000\n",
    "test_H0 = run_simulation(num_test_per_class, loc=0)\n",
    "test_H1 = run_simulation(num_test_per_class, loc=1)\n",
    "scores_H0 = clf.predict_proba(test_H0[['x']].values)[:, 1]\n",
    "scores_H1 = clf.predict_proba(test_H1[['x']].values)[:, 1]\n",
    "\n",
    "# Evaluate classifier scores on observed data\n",
    "scores_obs = clf.predict_proba(data_obs[['x']].values)[:, 1]\n",
    "\n",
    "# Plot histograms of the classifier scores\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(scores_H0, bins=100, range=(0,1), alpha=0.5, label='Simulation: H0', density=True)\n",
    "ax.hist(scores_H1, bins=100, range=(0,1), alpha=0.5, label='Simulation: H1', density=True)\n",
    "ax.hist(scores_obs, bins=100, range=(0,1), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.set_xlabel('Classifier score, $\\\\hat{f}(x)$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the observed classifier score distribution help identify the correct hypothesis? Let's do the full test...\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Using the likelihood-ratio trick, the classifier output can be converted to the density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)}\n",
    "$$\n",
    "\n",
    "With this, we can approximate the test-statistic over the full ($N_{obs}=10$) dataset ($\\mathcal{D}$):\n",
    "\n",
    "$$\n",
    "t \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "In the same way as the analytic approach, we will write python functions to calculate the log-likelihood ratio and the test-statistic. These functions evaluate the classifier on input data $\\{x_i\\} \\in \\mathcal{D}$, convert the classifier output to the log-likelihood-ratio per sample, and then calculate the test-statistic over the dataset by summing the values and multiplying by -2. Note, we clip the classifier output scores to avoid log(0) in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_clf(x, clf, clip=1e-10):\n",
    "    scores = clf.predict_proba(x.reshape(-1, 1))[:, 1]\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr\n",
    "\n",
    "def test_statistic_clf(x, clf):\n",
    "    llr = log_likelihood_ratio_clf(x, clf)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can perform the same hypothesis-testing procedure but now using the learned test-statistic. \n",
    "\n",
    "Let's generate pseudo-experiments under each hypothesis with $N_{obs}=10$. For each pseudo-experiment we will calculate the analytic and learned test-statistic values, building up distributions of each under the two hypotheses. We can then calculate the analytic and learned test-statistic values for the observed dataset, and calculate the $p$-values with respect to the null hypothesis.\n",
    "\n",
    "We will also compare the statistical power of the analytic vs learned test-statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0_clf = []\n",
    "test_statistic_H1_clf = []\n",
    "test_statistic_H0_analytic = []\n",
    "test_statistic_H1_analytic = []\n",
    "\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = run_simulation(N_obs, loc=0)\n",
    "    t_H0_clf = test_statistic_clf(samples_H0['x'].values, clf)\n",
    "    t_H0_analytic = test_statistic(samples_H0['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H0_clf.append(t_H0_clf)\n",
    "    test_statistic_H0_analytic.append(t_H0_analytic)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = run_simulation(N_obs, loc=1)\n",
    "    t_H1_clf = test_statistic_clf(samples_H1['x'].values, clf)\n",
    "    t_H1_analytic = test_statistic(samples_H1['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H1_clf.append(t_H1_clf)\n",
    "    test_statistic_H1_analytic.append(t_H1_analytic)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs_clf = test_statistic_clf(data_obs['x'].values, clf)\n",
    "t_obs_analytic = test_statistic(data_obs['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-values for H0\n",
    "p_value_H0_clf = np.sum(np.array(test_statistic_H0_clf) <= t_obs_clf) / N_toys\n",
    "p_value_H0_analytic = np.sum(np.array(test_statistic_H0_analytic) <= t_obs_analytic) / N_toys\n",
    "\n",
    "# Calculate the type-1 and type-2 errors for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "critical_value_analytic = np.percentile(test_statistic_H0_analytic, alpha * 100)\n",
    "type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "type_2_error_analytic = np.sum(np.array(test_statistic_H1_analytic) > critical_value_analytic) / N_toys\n",
    "power_clf = 1 - type_2_error_clf\n",
    "power_analytic = 1 - type_2_error_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results: compare analytic vs classifier-based test statistics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax[0].hist(test_statistic_H0_analytic, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H0 (analytic)', color='blue')\n",
    "ax[0].hist(test_statistic_H1_analytic, bins=50, alpha=0.5, range=(-50,50), label='Simulation: H1 (analytic)', color='orange')\n",
    "ax[0].axvline(t_obs_analytic, color='black', linestyle='solid', linewidth=2, label='Observed data (analytic): $p$={:.3f}'.format(p_value_H0_analytic))\n",
    "ax[0].hist(test_statistic_H0_clf, bins=50, range=(-50,50), label='Simulation: H0 (learned)', color='blue', histtype='step', linewidth=2)\n",
    "ax[0].hist(test_statistic_H1_clf, bins=50, range=(-50,50), label='Simulation: H1 (learned)', color='orange', histtype='step', linewidth=2)\n",
    "ax[0].axvline(t_obs_clf, color='grey', linestyle='dashed', linewidth=2, label='Observed data (learned): $p$={:.3f}'.format(p_value_H0_clf))\n",
    "\n",
    "ax[0].set_xlabel('t')\n",
    "ax[0].set_xlim(-75, 75)\n",
    "ax[0].set_ylabel('Number of pseudo-experiments')\n",
    "ax[0].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Add text to plot with type-1, type-2 errors and power for both methods\n",
    "textstr = '\\n'.join((\n",
    "    'Analytic:',\n",
    "    ' Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    ' Power (1-$\\\\beta$): {:.3f}'.format(power_analytic),\n",
    "    '',\n",
    "    'Learned:',\n",
    "    ' Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    ' Power (1-$\\\\beta$): {:.3f}'.format(power_clf)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "ax[0].text(0.05, 0.95, textstr, transform=ax[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Plot ROC curves for both methods\n",
    "alpha_vals = np.linspace(0.0, 1.0, 50)\n",
    "beta_vals_analytic = []\n",
    "beta_vals_clf = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value_analytic = np.percentile(test_statistic_H0_analytic, alpha * 100)\n",
    "    type_2_error_analytic = np.sum(np.array(test_statistic_H1_analytic) > critical_value_analytic) / N_toys\n",
    "    beta_vals_analytic.append(type_2_error_analytic)\n",
    "\n",
    "    critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "    type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "    beta_vals_clf.append(type_2_error_clf)\n",
    "\n",
    "fpr = alpha_vals\n",
    "tpr_analytic = 1 - np.array(beta_vals_analytic)\n",
    "tpr_clf = 1 - np.array(beta_vals_clf)\n",
    "\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc_analytic = np.trapezoid(tpr_analytic, fpr)\n",
    "auc_clf = np.trapezoid(tpr_clf, fpr)\n",
    "\n",
    "# Plot the ROC curve (beta vs alpha) for both methods\n",
    "ax[1].plot(fpr, tpr_analytic, label='Analytic test-statistic (AUC = {:.3f})'.format(auc_analytic), color='blue')\n",
    "ax[1].plot(fpr, tpr_clf, label='Learned test-statistic (AUC = {:.3f})'.format(auc_clf), color='orange', linestyle='dashed')\n",
    "ax[1].plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "ax[1].set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "ax[1].set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "ax[1].set_xlim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have \"learned\" a test-statistic which is essentially identical to the analytic solution. \n",
    "\n",
    "Instead, this was derived using a simple logistic regression binary classifier, trained with simulation. We have shown how to use the output of this classifier to perform a full hypothesis test. \n",
    "\n",
    "The observed data is inconsistent with the null hypothesis ($\\mathcal{H}_0$), with a $p$-value of less than 0.05. Therefore, under the two-class scenario, we conclude that the observed data support the $\\mathcal{H}_1$ hypothesis. \n",
    "\n",
    "In this simple 1D Gaussian example, we were able to use the analytic solution as a point of comparison. For most real-world scenarios, we do not have the analytic likelihood. In the next notebook, we will explore an example in which the analytic likelihood is not known."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
