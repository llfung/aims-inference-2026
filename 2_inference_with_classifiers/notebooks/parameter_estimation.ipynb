{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with classifiers\n",
    "\n",
    "# Parameter estimation: 2D Gaussian example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for notebook in Google Colab (otherwise not needed)\n",
    "import os\n",
    "\n",
    "# Install dependencies\n",
    "!pip install numpy pandas matplotlib scikit-learn torch --quiet\n",
    "\n",
    "# ---- Download utils.py ----\n",
    "if not os.path.exists(\"utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/utils.py\n",
    "\n",
    "# ---- Create data directory ----\n",
    "os.makedirs(\"data_observed\", exist_ok=True)\n",
    "\n",
    "# ---- Base raw URL ----\n",
    "base_url = \"https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/dev_inference_with_classifiers/2_inference_with_classifiers/notebooks/data_observed\"\n",
    "\n",
    "files = [\n",
    "    \"data_parameter_estimation.csv\",\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f\"data_observed/{f}\"):\n",
    "        !wget -q {base_url}/{f} -O data_observed/{f}\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will describe how to perform parameter estimation (with the extraction of confidence intervals) using ML classifiers. \n",
    "\n",
    "Note, this is essentially an extension of the hypothesis testing procedure where the alternative hypothesis $\\mathcal{H}_1$ is a \"composite\" hypothesis, containing the **ensemble** of all possible values for the parameter of interest. \n",
    "\n",
    "As discussed in the lecture, for parameter estimation we need to learn the conditional probability density: $p(x|\\theta)$.\n",
    "This quantity tells us the probabilty of observing data $x$ given parameter value $\\theta$. In fact, what we want to learn for inference with classifiers is the conditional density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{p(x|\\theta)}{p(x|\\theta_0)}\n",
    "$$\n",
    "\n",
    "where $\\theta_0$ is some reference value of the parameter. We can then define a log-likelihood-ratio test-statistic to infer $\\theta$ from the data, and its corresponding confidence intervals. All we need is a faithful simulator that can generate $x$ for any value of $\\theta$.\n",
    "\n",
    "### Parametric classifiers\n",
    "The parameter estimation with SBI discussed in this notebook is founded on the concept of parametric classifiers [1]. These models are trained such that the classifier output is \"parametric\" in $\\theta$: $\\hat{f}(x|\\theta)$ i.e. the classifier output depends on the value of $\\theta$. \n",
    "\n",
    "In practice, we learn this by simply adding $\\theta$ as an additional parameter to the model training:\n",
    "\n",
    "![parametric neural network](pnn.png)\n",
    "\n",
    "Crucially, one must initially make the distribution of the conditional feature ($\\theta$) the same between the two classes. \n",
    "\n",
    "### Training parametric classifiers for inference\n",
    "Let's go through the training steps in detail...\n",
    "\n",
    "1) Class 0 (**reference**), $\\mathcal{H}_0$: generate $N$ samples using the simulator for the reference hypothesis $\\theta = \\theta_0$ i.e. draw samples from $p(x|\\theta_0)$ for fixed $\\theta_0$. It is up to you what to choose for $\\theta_0$. In theory, the final parameter estimation will be independent of this choice, but it helps to pick something sensible which is not too far from the observed parameter values.\n",
    "\n",
    "2) Class 1 (**ensemble**), $\\mathcal{H}_1$: generate $N$ samples using the simulator with different values of $\\theta$ i.e. draw samples from $p(x|\\theta)$ for various $\\theta$. If possible, one should generate the samples to be continuous in the values of $\\theta$ i.e. randomly sampled between some upper and lower ranges, where the ranges are chosen to be sufficiently wide for the confidence interval level that you want to probe. In practice it is often much easier to generate subsamples with discrete steps in $\\theta$. The method still works as long as the discrete steps are sufficiently fine-grained (compared to the sensitivity). We will use the latter approach in this notebook.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "3) Consider training a classifier $\\hat{f}(x,\\theta)$ to distinguish between Class 0 and Class 1 with $\\{x,\\theta\\}$ as input features. After the likelihood-ratio trick, we arrive at an approximation of the joint-likelihood ratio between the two classes:\n",
    "    $$\n",
    "    \\frac{\\hat{f}(x_i,\\theta_i)}{1-\\hat{f}(x_i,\\theta_i)} \\approx \\frac{p_{\\mathcal{H}_1}(x_i,\\theta_i)}{p_{\\mathcal{H}_0}(x_i,\\theta_0)}\n",
    "    $$\n",
    "    If we expand the joint distributions into the product of the conditionals and the marginals we obtain:\n",
    "    $$\n",
    "    \\frac{p_{\\mathcal{H}_1}(x_i,\\theta_i)}{p_{\\mathcal{H}_0}(x_i,\\theta_0)}= \\frac{p(x_i|\\theta_i)p_{\\mathcal{H}_1}(\\theta_i)}{p(x_i|\\theta_0)p_{\\mathcal{H}_0}(\\theta_0)}\n",
    "    $$\n",
    "    What we want is only the conditional density ratio:\n",
    "    $$\n",
    "    \\frac{p(x_i|\\theta_i)}{p(x_i|\\theta_0)}\n",
    "    $$\n",
    "    Hence, we need to make sure the marginal distributions are the same between the two classes:\n",
    "    $$\n",
    "    p_{\\mathcal{H}_1}(\\theta) = p_{\\mathcal{H}_0}(\\theta)\n",
    "    $$\n",
    "    so that the terms cancel in the joint density ratio, and we are left with just the conditional density ratio. \n",
    "    \n",
    "    In other words, we need the $\\theta$ parameter distributions in Class 0 and Class 1 to have identical densities. \n",
    "    \n",
    "    In practice, this means pairing each data sample $x_i$ from Class 0 with a randomly sampled $\\theta_i$ value from the Class 1 distribution. If they differ, the classifier will pick up class-specific information about how often certain $\\theta$ values appear in each class, rather than purely how likely the data $x$ are given $\\theta$. This would break the interpretation of the classifier output as a conditional density ratio.\n",
    "\n",
    "    So (in summary) before training we need to artificially generate a $\\theta$ distribution for Class 0 by sampling from the Class 1 distribution. \n",
    "\n",
    "</div> \n",
    "\n",
    "4) After the $\\theta$ distributions between the two classes are made to be the same in training, the classifier output can be used to approximate the conditional density ratio. We will show later on how this can be used for parameter estimation.\n",
    "\n",
    "[1] - P. Baldi et al., *Parameterized neural networks for high-energy physics*, Eur. Phys. J. C 76 (2016) 235. [arXiv:1601.07913](https://www.arxiv.org/abs/1601.07913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Gaussian example\n",
    "\n",
    "In this notebook we will follow a simple example, **where the analytic likelihood is known**. We will then compare the \"learned\" conditional density ratio to the analytic result.\n",
    "\n",
    "The example is described below:\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "Two observables ($x_1, x_2$) which follow a 2D Gaussian distribution. The mean and width related to $x_1$, $\\mu_1$ and $\\sigma_1$, are known. Only the width related to $x_2$, $\\sigma_2$, is known. The two unknowns are $\\mu_2$ and the correlation between the observables $\\rho_{12}$. We are provided with a dataset of $N_{obs}=20$ samples of ($x_1, x_2$) values stored in `data_observed/data_parameter_estimation.csv`. \n",
    "\n",
    "### Aim:\n",
    "\n",
    "Infer the values of $\\mu_2$ and $\\rho_{12}$ from the data, along with the corresponding confidence intervals. Comparing to the notation above $x = \\{x_1,x_2\\}$ and $\\theta = \\{\\mu_2,\\rho_{12}\\}$.\n",
    "\n",
    "### Simulation \n",
    "\n",
    "We have a simulator that can generate samples from a 2D Gaussian with arbitrary $\\mu_1$, $\\sigma_1$, $\\mu_2$, $\\sigma_2$, $\\rho_{12}$. The simulation can generate $N$ samples using the following code:\n",
    "```python\n",
    "sim = run_simulation(N, mu1, sigma1, mu2, sigma2, rho12)\n",
    "```\n",
    "\n",
    "### Analytic solution\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The analytic (conditional) probability density for a 2D Gaussian is:\n",
    "\n",
    "$$\n",
    "p(x_1,x_2|\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,\\rho_{12})\n",
    "=\n",
    "\\frac{1}{2\\pi\\,\\sigma_1\\sigma_2\\sqrt{1-\\rho_{12}^2}}\n",
    "\\exp\\!\\left(\n",
    "-\\frac{1}{2(1-\\rho_{12}^2)}\n",
    "\\left[\n",
    "\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}\n",
    "+\n",
    "\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}\n",
    "-\n",
    "\\frac{2\\rho_{12}(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}\n",
    "\\right]\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "We will use this to compare to the \"learned\" conditional density ratio.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Run the simulation and compare to the observed data\n",
    "1. Extract analytic result\n",
    "1. Train a parametric classifier\n",
    "1. Use parametric classifier output to estimate the values and confidence intervals of $\\mu_2$ and $\\rho_{12}$.\n",
    "\n",
    "## Run simulation and data exploration\n",
    "We will begin by writing a python function to run our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(N, mu1=0, sigma1=1, mu2=0, sigma2=1, rho12=0):\n",
    "    cov_matrix = [[sigma1**2, rho12 * sigma1 * sigma2],\n",
    "                  [rho12 * sigma1 * sigma2, sigma2**2]]\n",
    "    data = np.random.multivariate_normal(mean=[mu1, mu2], cov=cov_matrix, size=N)\n",
    "    df = pd.DataFrame(data, columns=['x1', 'x2'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pick a sensible reference hypothesis ($\\theta=\\theta_0$). For this example, we will pick a 2D unit Gaussian with zero correlation.\n",
    "\n",
    "Let's run the simulation for our reference sample ($\\mathcal{H}_0$) and for a potential alternative hypothesis (from the ensemble of alternative hypotheses). In this example we know $\\mu_1=0, \\sigma_1=1$ and $\\sigma_2=1$, so we only vary $\\mu_2$ and $\\rho_{12}$ in the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "num_train_per_class = 100000\n",
    "\n",
    "# H0: reference sample\n",
    "sim_H0 = run_simulation(num_train_per_class, mu1=0, sigma1=1, mu2=0, sigma2=1, rho12=0)\n",
    "\n",
    "# H1: example alternative hypothesis\n",
    "sim_H1 = run_simulation(num_train_per_class, mu1=0, sigma1=1, mu2=0.5, sigma2=1, rho12=0.5)\n",
    "\n",
    "# Load the data from csv file\n",
    "data_obs = pd.read_csv('data_observed/data_parameter_estimation.csv')\n",
    "N_obs = len(data_obs)\n",
    "data_obs.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D heatmaps for (x1,x2) and overlay observed data\n",
    "# Show for both reference sample and example alternative hypothesis\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "axs[0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('x2')\n",
    "axs[0].set_title('Simulation: H0 (reference)')\n",
    "axs[0].legend()\n",
    "# Add text box with parameter values for reference sample\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\mu_1=0$ (known)',\n",
    "    r'$\\sigma_1=1$ (known)',\n",
    "    r'$\\mu_2=0$',\n",
    "    r'$\\sigma_2=1$ (known)',\n",
    "    r'$\\rho_{12}=0$'))\n",
    "axs[0].text(0.05, 0.95, textstr, transform=axs[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "axs[1].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[1].set_xlabel('x1')\n",
    "axs[1].set_ylabel('x2')\n",
    "axs[1].set_title('Simulation: H1 (example alternative)')\n",
    "axs[1].legend()\n",
    "# Add text box with parameter values for example alternative hypothesis\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\mu_1=0$ (known)',\n",
    "    r'$\\sigma_1=1$ (known)',\n",
    "    r'$\\mu_2=0.5$',\n",
    "    r'$\\sigma_2=1$ (known)',\n",
    "    r'$\\rho_{12}=0.5$'))\n",
    "axs[1].text(0.05, 0.95, textstr, transform=axs[1].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data look more like the reference hypothesis or the example alternative hypothesis? It is hard to tell. Let's perform a proper parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic calculation of test statistic\n",
    "We will use the analytic form of the density to calculate the log-likelihood ratio. With this formula, we can calculate the test-statistic $t$ as a function of $(\\mu_2,\\rho_{12})$ for the observed data. From this we can determine the best-fit value (minimum) and the corresponding confidence intervals.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "With $(\\mu_1, \\sigma_1, \\sigma_2)$ known, the probability density becomes:\n",
    "\n",
    "$$\n",
    "p(x|\\mu_2,\\rho_{12})\n",
    "=\n",
    "\\frac{1}{2\\pi\\sqrt{1-\\rho_{12}^2}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2(1-\\rho_{12}^2)}\n",
    "\\left[\n",
    "x_1^2\n",
    "+\n",
    "(x_2-\\mu_2)^2\n",
    "-\n",
    "2\\rho_{12}x_1(x_2-\\mu_2)\n",
    "\\right]\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The test-statistic is defined as the log-likelihood ratio between the reference hypothesis and the best-fit alternative hypothesis. We can calculate this for a given dataset $\\mathcal{D}$ for any value of $(\\mu_2,\\rho_{12})$:\n",
    "\n",
    "$$\n",
    "t(\\mathcal{D}|\\mu_2,\\rho_{12}) = -2 \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\frac{p(x_i|\\mu_2,\\rho_{12})}{p(x_i|0,0)}}\n",
    "$$\n",
    "\n",
    "The best-fit value of $(\\mu_2,\\rho_{12})$ is the one that minimizes $t$ (maximum-likelihood estimate). We then subtract the minimum value of $t$ so that the minimum is zero:\n",
    "\n",
    "$$\n",
    "\\Delta t(\\mathcal{D}|\\mu_2,\\rho_{12}) = t(\\mathcal{D}|\\mu_2,\\rho_{12}) - t(\\mathcal{D}|\\hat{\\mu}_2,\\hat{\\rho}_{12})\n",
    "$$\n",
    "\n",
    "We will use the $\\Delta t$ test-statistic to determine the confidence intervals.\n",
    "\n",
    "</div>\n",
    "\n",
    "Make sure you understand the form of the python function below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_analytic(x, params, params_ref):\n",
    "    mu1, sigma1, mu2, sigma2, rho12 = params\n",
    "    mu1_ref, sigma1_ref, mu2_ref, sigma2_ref, rho12_ref = params_ref\n",
    "\n",
    "    cov_matrix = np.array([[sigma1**2, rho12 * sigma1 * sigma2],\n",
    "                           [rho12 * sigma1 * sigma2, sigma2**2]])\n",
    "    cov_matrix_ref = np.array([[sigma1_ref**2, rho12_ref * sigma1_ref * sigma2_ref],\n",
    "                               [rho12_ref * sigma1_ref * sigma2_ref, sigma2_ref**2]])\n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov_matrix)\n",
    "    inv_cov_ref = np.linalg.inv(cov_matrix_ref)\n",
    "    \n",
    "    diff = x - np.array([mu1, mu2])\n",
    "    diff_ref = x - np.array([mu1_ref, mu2_ref])\n",
    "\n",
    "    ll = -0.5 * np.einsum('ij,jk,ik->i', diff, inv_cov, diff) - 0.5 * np.log(np.linalg.det(cov_matrix)) - np.log(2 * np.pi)\n",
    "    ll_ref = -0.5 * np.einsum('ij,jk,ik->i', diff_ref, inv_cov_ref, diff_ref) - 0.5 * np.log(np.linalg.det(cov_matrix_ref)) - np.log(2 * np.pi)\n",
    "    return ll-ll_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan over mu2 and rho12 values and calculate llr for observed data at each point\n",
    "mu2_points = np.linspace(-1, 1, 100)\n",
    "rho12_points = np.linspace(-0.99, 0.99, 100)\n",
    "llr_vals = np.zeros((len(mu2_points), len(rho12_points)))\n",
    "params_ref = (0, 1, 0, 1, 0)  # Reference parameters (H0)\n",
    "for i, mu2 in enumerate(mu2_points):\n",
    "    for j, rho12 in enumerate(rho12_points):\n",
    "        params = (0, 1, mu2, 1, rho12)  # Current parameters\n",
    "        llr = log_likelihood_ratio_analytic(data_obs[['x1', 'x2']].values, params, params_ref)\n",
    "        llr_vals[i, j] = np.sum(llr)\n",
    "\n",
    "# Convert llr to test statistic\n",
    "test_statistic_analytic = -2 * (llr_vals-np.max(llr_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot $\\Delta t(\\mu_2,\\rho_{12})$ in the $(\\mu_2,\\rho_{12})$ plane. As mentioned above, the best-fit parameter value corresponds to the minimum of the test-statistic curve (now at zero by construction).\n",
    "\n",
    "Using Wilk's theorem [1], this test-statistic distribution asymptotically approaches the $\\chi^2$-distribution with $n=2$ degrees of freedom. We will use this \"asymptotic approximation\" to estimate the 68% and 95% confidence intervals. These are the union of points in the plane for which $t$ is less than 2.3 and 5.99, respectively. In principle, one should check the validity of Wilk's theorem with the Neyman construction of confidence intervals (using pseudo-data), but we will not go into that in this notebook.\n",
    "\n",
    "Let's plot the test-statistic surface, and highlight the best-fit, 68% and 95% CL intervals for the $(\\mu_2,\\rho_{12})$ parameters. \n",
    "\n",
    "[1] - Wilk's Theorem. [Wikipedia link](https://en.wikipedia.org/wiki/Wilks%27_theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "# Set limit on color scale as t=20\n",
    "c = ax.imshow(test_statistic_analytic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax.set_xlabel(r'$\\mu_2$')\n",
    "ax.set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c, ax=ax, label=r'$\\Delta t(\\mathcal{D}|\\mu_2,\\rho_{12}) = t(\\mathcal{D}|\\mu_2,\\rho_{12}) - t(\\mathcal{D}|\\hat{\\mu}_2,\\hat{\\rho}_{12})$')\n",
    "# Overlay contour lines\n",
    "contours = ax.contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles='dashed')\n",
    "ax.clabel(contours, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "# Add minimum point\n",
    "min_idx = np.unravel_index(np.argmin(test_statistic_analytic), test_statistic_analytic.shape)\n",
    "ax.plot(mu2_points[min_idx[0]], rho12_points[min_idx[1]], marker='x', color='red', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data favours a positive correlation ($\\rho_{12}$), with a mean $\\mu_2$ around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a parametric classifier\n",
    "Now we will perform the parameter estimation using SBI.\n",
    "\n",
    "As described above we need to generate samples for Class 1 (ensemble) i.e. where we draw samples from the simulator for various values of $\\theta = \\{\\mu_2,\\rho_{12}\\}$. \n",
    "\n",
    "For this, we will create subsamples with small discrete points in a grid of $\\mu_2$ and $\\rho_{12}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_H1_ensemble = []\n",
    "mu2_train_vals = np.linspace(-1,1,50)\n",
    "rho12_train_vals = np.linspace(-0.9999,0.9999,50) \n",
    "# Calculate the number of training samples per parameter point to keep total = num_train_per_class\n",
    "num_train_per_subsample = num_train_per_class // (len(mu2_train_vals) * len(rho12_train_vals))\n",
    "for mu2 in mu2_train_vals:\n",
    "    for rho12 in rho12_train_vals:\n",
    "        sim_subsample = run_simulation(num_train_per_subsample, mu1=0, sigma1=1, mu2=mu2, sigma2=1, rho12=rho12)\n",
    "        sim_subsample['mu2'] = mu2\n",
    "        sim_subsample['rho12'] = rho12\n",
    "        sim_H1_ensemble.append(sim_subsample)\n",
    "\n",
    "# Concatenate all subsamples into a single dataframe and add label\n",
    "sim_H1 = pd.concat(sim_H1_ensemble, ignore_index=True)\n",
    "sim_H1['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step is crucial to ensure that we learn the conditional density ratio. We need to match the marginal distributions of the conditional parameters $\\mu_2,\\rho_{12}$ in Class 0 (reference) to Class 1. This ensures that the classifier will not pick up class-specific information about how often certain $\\theta$ values appear in each class, and instead will learn how likely the data $x$ are given $\\theta$.\n",
    "\n",
    "To do this, for each sample in Class 0 we randomly choose $\\mu_2,\\rho_{12}$ values from the possible discrete values used to generate Class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_H0['mu2'] = np.random.choice(mu2_train_vals, size=len(sim_H0))\n",
    "sim_H0['rho12'] = np.random.choice(rho12_train_vals, size=len(sim_H0))\n",
    "\n",
    "# Add label for H0\n",
    "sim_H0['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explicitly check that the conditional parameter distributions are identical (give or take statistical fluctuations)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].hist(sim_H0['mu2'], bins=50, range=(-1,1), alpha=0.5, label='Simulation: Class 0')\n",
    "ax[0].hist(sim_H1['mu2'], bins=50, range=(-1,1), alpha=0.5, label='Simulation: Class 1')\n",
    "ax[0].set_xlabel(r'$\\mu_2$')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_ylim(0, ax[0].get_ylim()[1]*1.2)\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].hist(sim_H0['rho12'], bins=50, range=(-1,1), alpha=0.5, label='Simulation: Class 0')\n",
    "ax[1].hist(sim_H1['rho12'], bins=50, range=(-1,1), alpha=0.5, label='Simulation: Class 1')\n",
    "ax[1].set_xlabel(r'$\\rho_{12}$')\n",
    "ax[1].set_ylabel('Counts')\n",
    "ax[1].set_ylim(0, ax[1].get_ylim()[1]*1.2)\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now that the conditional parameter distributions are the same (give or take some statistical fluctuations), we can correctly train a parametric classifier for infernece.\n",
    "\n",
    "We will again use a feed-forward MLP for this (relatively) simple example.\n",
    "\n",
    "Let's first prepare the datasets for training and then build the model. Note, the model takes as input both $(x_1,x_2)$ and $(\\mu_2,\\rho_{12})$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets and split into train/test sets\n",
    "sim = pd.concat([sim_H0, sim_H1], ignore_index=True)\n",
    "test_size = 0.2\n",
    "sim_train, sim_test = train_test_split(sim, test_size=test_size, shuffle=True)\n",
    "\n",
    "# Function to convert pandas dataframe to a torch tensor\n",
    "def df_to_tensor(df, feature_cols):\n",
    "    return torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "# Convert to torch tensors: notice how theta are used as input features\n",
    "feature_cols = ['x1', 'x2', 'mu2', 'rho12']\n",
    "X_train = df_to_tensor(sim_train, feature_cols)\n",
    "y_train = df_to_tensor(sim_train, ['label'])\n",
    "X_test = df_to_tensor(sim_test, feature_cols)\n",
    "y_test = df_to_tensor(sim_test, ['label'])\n",
    "# Create DataLoader for batching\n",
    "batch_size = 2048\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with optional number of hidden layers and units\n",
    "class ParameterizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers=2):\n",
    "        super(ParameterizedMLP, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "# Initialize model, loss function and optimizer\n",
    "input_size = 4  # x1, x2, mu2, rho12\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 4\n",
    "model = ParameterizedMLP(input_size, hidden_size, num_hidden_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * inputs.size(0)\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing loss curves\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the parametric classifier for inference\n",
    "Now that the model has been trained, we can perform the parameter estimation.\n",
    "\n",
    "This is done by **evaluating the classifier output for the observed data sample** over different values of the parameters of interest: $\\hat{f}(x_i|\\mu_2,\\rho_{12})$.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Using the likelihood-ratio-trick, we obtain an approximation of the conditional density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i|\\mu_2,\\rho_{12})}{1-\\hat{f}(x_i|\\mu_2,\\rho_{12})} \\approx \\frac{p(x_i|\\mu_2,\\rho_{12})}{p(x_i|\\mu_2=0,\\rho_{12}=0)}\n",
    "$$\n",
    "\n",
    "Just like in the hypothesis-testing procedure, we can use this to approximate the log-likelihood-ratio test statistic over the whole data set:\n",
    "\n",
    "$$\n",
    "t(\\mathcal{D}|\\mu_2,\\rho_{12}) \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i|\\mu_2,\\rho_{12})}{1-\\hat{f}(x_i|\\mu_2,\\rho_{12})}\\bigg)}\n",
    "$$\n",
    "\n",
    "where crucially our learned test statistic is now a function of (i.e. conditional on) the parameters of interest. \n",
    "\n",
    "</div>\n",
    "\n",
    "We evaluate the learned test statistic values in a grid of $(\\mu_2,\\rho_{12})$ to build up the test statistic surface. We can then use this surface to infer the best-fit parameter values (minimum) and their respective confidence intervals.\n",
    "\n",
    "The final test-statistic is shifted so that the minimum is at zero:\n",
    "\n",
    "$$\n",
    "\\Delta t(\\mathcal{D}|\\mu_2,\\rho_{12}) = t(\\mathcal{D}|\\mu_2,\\rho_{12}) - t(\\mathcal{D}|\\hat{\\mu}_2,\\hat{\\rho}_{12})\n",
    "$$\n",
    "\n",
    "Let's again write python functions to calculate the test-statistic using the classifier output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_parameterized(X, mu2_eval, rho12_eval, model, clip=1e-10):\n",
    "    N = X.shape[0]\n",
    "    mu2_array = np.full(N, mu2_eval)\n",
    "    rho12_array = np.full(N, rho12_eval)\n",
    "    input_array = np.column_stack((X, mu2_array, rho12_array))\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.tensor(input_array, dtype=torch.float32)).numpy().flatten()\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we scan over the $(\\mu_2,\\rho_{12})$ plane but now calculate the learned test statistic for each point. We will then compare this to the result from the analytic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu2_points = np.linspace(-1, 1, 100)\n",
    "rho12_points = np.linspace(-0.99, 0.99, 100)\n",
    "llr_vals_clf = np.zeros((len(mu2_points), len(rho12_points)))\n",
    "for i, mu2 in enumerate(mu2_points):\n",
    "    for j, rho12 in enumerate(rho12_points):\n",
    "        llr = log_likelihood_ratio_parameterized(data_obs[['x1', 'x2']].values, mu2, rho12, model)\n",
    "        llr_vals_clf[i, j] = np.sum(llr)\n",
    "\n",
    "# Convert llr to test statistic\n",
    "test_statistic_clf = -2 * (llr_vals_clf-np.max(llr_vals_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test-statistic heatmap from parameterized classifier next to analytic solution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "c1 = ax[0].imshow(test_statistic_analytic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax[0].set_xlabel(r'$\\mu_2$')\n",
    "ax[0].set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c1, ax=ax[0], label=r'Analytic test statistic $t$')\n",
    "contours1 = ax[0].contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles=['solid', 'dashed'])\n",
    "ax[0].clabel(contours1, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "min_idx_analytic = np.unravel_index(np.argmin(test_statistic_analytic), test_statistic_analytic.shape)\n",
    "ax[0].plot(mu2_points[min_idx_analytic[0]], rho12_points[min_idx_analytic[1]], marker='x', color='red', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax[0].legend()\n",
    "\n",
    "c2 = ax[1].imshow(test_statistic_clf.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "                origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax[1].set_xlabel(r'$\\mu_2$')\n",
    "ax[1].set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c2, ax=ax[1], label=r'Learned test statistic $t$')\n",
    "contours2 = ax[1].contour(mu2_points, rho12_points, test_statistic_clf.T, levels=[2.30, 5.99], colors='black', linestyles=['solid', 'dashed'])\n",
    "ax[1].clabel(contours2, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "min_idx_clf = np.unravel_index(np.argmin(test_statistic_clf), test_statistic_clf.shape)\n",
    "ax[1].plot(mu2_points[min_idx_clf[0]], rho12_points[min_idx_clf[1]], marker='x', color='black', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise how well we have done in learning the test statistic by taking the difference of the test statistic surfaces, or by comparing the contours on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference of test-statistic heatmaps next to plot with all contours on same axis\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "diff_test_statistic = test_statistic_clf - test_statistic_analytic\n",
    "c = ax.imshow(diff_test_statistic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='BrBG', vmin=-5, vmax=5)\n",
    "ax.set_xlabel(r'$\\mu_2$')\n",
    "ax.set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c, ax=ax, label=r'Difference in Test Statistic $t_{clf} - t_{analytic}$')\n",
    "# Add contours for reference\n",
    "contours_analytic = ax.contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles=['solid', 'dashed'])\n",
    "contours_clf = ax.contour(mu2_points, rho12_points, test_statistic_clf.T, levels=[2.30, 5.99], colors='blue', linestyles=['solid', 'dashed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have done a reasonable job at approximating the test statistic surface. This allows us to infer $\\mu_2$ and $\\rho_{12}$ from the observed data without a-priori knowing the likelihood! \n",
    "\n",
    "For reference, the true values used to generate `data_observed/data_parameter_estimation.csv` were $\\mu^{\\mathrm{true}}_2 = -0.1$ and $\\rho^{\\mathrm{true}}_{12} = 0.3$. This is consistent with our SBI measurement within the 68% confidence level interval. \n",
    "\n",
    "That said, our SBI model is not perfect and there is room for improvement. Differences in the learned likelihood-ratio compared to the true likelihood-ratio will lead to a biased estimator, and therefore can lead to biased measurements. In a worst-case scenario, this may lead to incorrect conclusions! In Extension 1 below, you will explore how to improve the training procedure to provide a more accurate test-statistic prediction. Ultimately, when using SBI in real research settings, significant time is spent validating the SBI models to ensure that we do not bias our measurements.\n",
    "\n",
    "Although this example (2D Gaussian with two unknown parameters) is overkill, we have demonstrated all the steps necessary to perform parameter estimation with a ML classifier. You can imagine how useful this becomes for more complex problems when the true likelihood is not known. We can leverage the benefits of machine learning to perform inference in high-dimensions in a complex feature space, thereby squeezing every drop of information out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Extensions\n",
    "\n",
    "### 1. Improving the accuracy of the parameter estimation\n",
    "In the example above the \"learned\" test statistic shows some deviations from the analytic solution. This could lead to biases (or incorrect conclusions) when using the ML SBI approach. Can you improve the accuracy by altering the model training? One could consider:\n",
    "* Increasing the amount of training data\n",
    "* Increasing the granularity of the $(\\mu_2,\\rho_{12})$ grid used for the training\n",
    "* Extending to a more complex model arhcitecture\n",
    "\n",
    "Reproduce the plots at the end of this section to see how your changes affect the accuracy.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### 2. Validating the classifier\n",
    "It is extremely important to check that the best-fit and confidence intervals used to generate simulated data are consistent with the results obtained from the neural SBI procedure.\n",
    "\n",
    "Try generating data for a particular $(\\mu_2,\\rho_{12})$ s, starting with $N=20$ samples, and perform inference on this new dataset. \n",
    "\n",
    "Then generate $N=50$ samples and repeat the inference. How do the confidence intervals change? Is the \"true\" value used to generate the samples consistent with the results? \n",
    "\n",
    "How does this change for $N=100$, $N=500$ etc? Do you eventually observe a sizeable bias in the results?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define true values of parameters\n",
    "mu2_true = -0.1\n",
    "rho12_true = 0.3\n",
    "\n",
    "# Generate pseudo-experiments and compute test statistic at true parameter values\n",
    "N_obs_vals = [10, 20, 50, 100, 500]\n",
    "data_val = {}\n",
    "for N_obs in N_obs_vals:\n",
    "    data_val[N_obs] = run_simulation(N_obs, mu1=0, sigma1=1, mu2=mu2_true, sigma2=1, rho12=rho12_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test statistic grid for both analytic and parameterized methods for each N_obs\n",
    "# This cell may take a few minutes to run\n",
    "mu2_vals = np.linspace(-1, 1, 100)\n",
    "rho12_vals = np.linspace(-0.9999, 0.9999, 100)\n",
    "\n",
    "test_statistic_analytic_obs = {}\n",
    "test_statistic_param_obs = {}\n",
    "llr_vals_analytic_obs = {}\n",
    "llr_vals_param_obs = {}\n",
    "\n",
    "for N_obs in N_obs_vals:\n",
    "    llr_vals_analytic_obs[N_obs] = np.zeros((len(mu2_vals), len(rho12_vals)))\n",
    "    llr_vals_param_obs[N_obs] = np.zeros((len(mu2_vals), len(rho12_vals)))\n",
    "\n",
    "for i, mu2 in enumerate(mu2_vals):\n",
    "    for j, rho12 in enumerate(rho12_vals):\n",
    "        params = (0, 1, mu2, 1, rho12)\n",
    "        params_ref = (0, 1, 0, 1, 0)\n",
    "        for N_obs in N_obs_vals:\n",
    "            llr_analytic = log_likelihood_ratio_analytic(data_val[N_obs][['x1', 'x2']].values, params, params_ref)\n",
    "            llr_vals_analytic_obs[N_obs][i, j] = np.sum(llr_analytic)\n",
    "\n",
    "            llr_param = log_likelihood_ratio_parameterized(data_val[N_obs][['x1', 'x2']].values, mu2, rho12, model)\n",
    "            llr_vals_param_obs[N_obs][i, j] = np.sum(llr_param)\n",
    "\n",
    "for N_obs in N_obs_vals:\n",
    "    test_statistic_analytic_obs[N_obs] = -2 * (llr_vals_analytic_obs[N_obs] - np.max(llr_vals_analytic_obs[N_obs]))\n",
    "    test_statistic_param_obs[N_obs] = -2 * (llr_vals_param_obs[N_obs] - np.max(llr_vals_param_obs[N_obs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confidence interval contours for both methods at different N_obs\n",
    "fig, axs = plt.subplots(len(N_obs_vals), 2, figsize=(12, 4*len(N_obs_vals)))\n",
    "for idx, N_obs in enumerate(N_obs_vals):\n",
    "    # Analytic method\n",
    "    axs[idx, 0].set_title(f'Analytic Method (N_obs={N_obs})')\n",
    "    axs[idx, 0].set_xlabel(r'$\\mu_2$')\n",
    "    axs[idx, 0].set_ylabel(r'$\\rho_{12}$')\n",
    "    contours1 = axs[idx, 0].contour(mu2_vals, rho12_vals, test_statistic_analytic_obs[N_obs].T, levels=[2.30, 5.99], colors='red', linestyles=['solid', 'dashed'])\n",
    "    axs[idx, 0].clabel(contours1, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "    # Add marker for true parameter values\n",
    "    axs[idx, 0].plot(mu2_true, rho12_true, marker='x', color='black', markersize=5, label='True Value', linestyle='None')\n",
    "    # Add marker for best-fit parameter values\n",
    "    min_idx_analytic = np.unravel_index(np.argmin(test_statistic_analytic_obs[N_obs]), test_statistic_analytic_obs[N_obs].shape)\n",
    "    axs[idx, 0].plot(mu2_vals[min_idx_analytic[0]], rho12_vals[min_idx_analytic[1]], marker='x', color='red', markersize=5, label='Best Fit', linestyle='None')\n",
    "    axs[idx, 0].legend()\n",
    "\n",
    "    # Parameterized method\n",
    "    axs[idx, 1].set_title(f'Parameterized Classifier (N_obs={N_obs})')\n",
    "    axs[idx, 1].set_xlabel(r'$\\mu_2$')\n",
    "    axs[idx, 1].set_ylabel(r'$\\rho_{12}$')\n",
    "    contours2 = axs[idx, 1].contour(mu2_vals, rho12_vals, test_statistic_param_obs[N_obs].T, levels=[2.30, 5.99], colors='blue', linestyles=['solid', 'dashed'])\n",
    "    axs[idx, 1].clabel(contours2, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "    # Add marker for true parameter values\n",
    "    axs[idx, 1].plot(mu2_true, rho12_true, marker='x', color='black', markersize=5, label='True Value', linestyle='None')\n",
    "    # Add marker for best-fit parameter values\n",
    "    min_idx_param = np.unravel_index(np.argmin(test_statistic_param_obs[N_obs]), test_statistic_param_obs[N_obs].shape)\n",
    "    axs[idx, 1].plot(mu2_vals[min_idx_param[0]], rho12_vals[min_idx_param[1]], marker='x', color='blue', markersize=5, label='Best Fit', linestyle='None')\n",
    "    axs[idx, 1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be doing a reasonable job! We may find as we go higher in the number of observed samples that the confidence intervals become more different, and we may start to see a bias in the results.\n",
    "\n",
    "Note, we have not had to retrain the model for the new datasets... this is a key advantage of neural SBI = amortized inference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
