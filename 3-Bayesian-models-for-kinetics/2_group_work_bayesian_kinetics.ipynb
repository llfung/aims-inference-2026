{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Bayesian Hybrid Models for Enzyme Kinetics - Group Exercise\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jonathon-langford/aims-inference-2026/blob/main/3-Bayesian-models-for-kinetics/2_group_work_bayesian_kinetics.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "1. Build and compare mechanistic vs. hybrid Bayesian models\n",
    "2. Diagnose MCMC convergence using trace plots, $\\hat{R}$, and ESS\n",
    "3. Use posterior predictive checks to assess model adequacy\n",
    "4. Visualise and interpret prediction uncertainty\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will analyse enzyme kinetics data where the reaction rate constant k depends on experimental conditions: temperature (T) and pH. \n",
    "\n",
    "The challenge: Can a hybrid model (mechanistic ODE + Gaussian process) outperform a simple mechanistic model with constant k?\n",
    "\n",
    "## Task Distribution (6 students, feel free to redistribute as you like)\n",
    "\n",
    "- **Task 1**: Check convergence for baseline model\n",
    "- **Task 2**: Posterior predictive checks for baseline\n",
    "- **Task 3**: Build the hybrid GP model\n",
    "- **Task 4**: Check convergence and visualize learned k(T, pH)\n",
    "- **Task 5**: Compare models with posterior predictive checks\n",
    "- **Task 6**: Experiment with kernels and length scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf040d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pymc arviz matplotlib numpy pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kinetic data\n",
    "url = \"https://raw.githubusercontent.com/jonathon-langford/aims-inference-2026/4cba750e6017d7d4b236aef029b556e04369fea2/3-Bayesian-models-for-kinetics/kinetic_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(\"Data Overview:\")\n",
    "print(f\"Total observations: {len(data)}\")\n",
    "print(f\"Unique samples: {data['SampleID'].nunique()}\")\n",
    "print(f\"Time points per sample: {data.groupby('SampleID').size().iloc[0]}\")\n",
    "print(f\"\\nTemperature range: {data['T'].min():.1f} - {data['T'].max():.1f} °C\")\n",
    "print(f\"pH range: {data['pH'].min():.2f} - {data['pH'].max():.2f}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a6166",
   "metadata": {},
   "source": [
    "### Visualise sample time courses at different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_ids = rng.choice(data[\"SampleID\"].unique(), size=6, replace=False)\n",
    "for idx, sid in enumerate(sample_ids):\n",
    "    ax = axes[idx]\n",
    "    subset = data[data[\"SampleID\"] == sid]\n",
    "    T_val = subset[\"T\"].iloc[0]\n",
    "    pH_val = subset[\"pH\"].iloc[0]\n",
    "    \n",
    "    ax.plot(subset[\"Time\"], subset[\"P_obs\"], \"o-\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    ax.set_ylabel(\"Product [P]\")\n",
    "    ax.set_title(f\"Sample {sid}: T={T_val:.1f}°C, pH={pH_val:.2f}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Questions:\")\n",
    "print(\"1. Do all samples reach the same plateau? (Hint: use sharey in plots)\")\n",
    "print(\"2. Do some samples show faster reaction rates than others?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815b34e",
   "metadata": {},
   "source": [
    "### Visualise distribution of experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sample_conditions = data.groupby(\"SampleID\")[[\"T\", \"pH\"]].first()\n",
    "\n",
    "axes[0].hist(sample_conditions[\"T\"], bins=10, edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Temperature (°C)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Temperature Distribution\")\n",
    "\n",
    "axes[1].hist(sample_conditions[\"pH\"], bins=10, edgecolor=\"black\")\n",
    "axes[1].set_xlabel(\"pH\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"pH Distribution\")\n",
    "\n",
    "axes[2].scatter(sample_conditions[\"T\"], sample_conditions[\"pH\"], alpha=0.6)\n",
    "axes[2].set_xlabel(\"Temperature (°C)\")\n",
    "axes[2].set_ylabel(\"pH\")\n",
    "axes[2].set_title(\"Experimental Design Space\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reshape_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: Reshape Data for Modeling\n",
    "\n",
    "We reshape the data into matrices for efficient modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reshape_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique samples and times\n",
    "unique_samples = data[[\"SampleID\", \"T\", \"pH\"]].drop_duplicates().reset_index(drop=True)\n",
    "unique_times = np.sort(data[\"Time\"].unique())\n",
    "\n",
    "n_samples = len(unique_samples)\n",
    "n_timepoints = len(unique_times)\n",
    "\n",
    "print(f\"Number of unique samples: {n_samples}\")\n",
    "print(f\"Number of unique timepoints: {n_timepoints}\")\n",
    "\n",
    "# Map SampleID to index\n",
    "sample_index = {sid: i for i, sid in enumerate(unique_samples[\"SampleID\"])}\n",
    "data[\"sample_idx\"] = data[\"SampleID\"].map(sample_index)\n",
    "\n",
    "# Create observation matrix (samples x timepoints)\n",
    "P_obs_matrix = np.full((n_samples, n_timepoints), np.nan)\n",
    "for i, sid in enumerate(unique_samples[\"SampleID\"]):\n",
    "    subset = data[data[\"SampleID\"] == sid].sort_values(\"Time\")\n",
    "    P_obs_matrix[i, :] = subset[\"P_obs\"].values\n",
    "\n",
    "print(f\"\\nObservation matrix shape: {P_obs_matrix.shape}\")\n",
    "\n",
    "# Normalise features for GP\n",
    "T_mean = unique_samples[\"T\"].mean()\n",
    "T_std_val = unique_samples[\"T\"].std()\n",
    "pH_mean = unique_samples[\"pH\"].mean()\n",
    "pH_std_val = unique_samples[\"pH\"].std()\n",
    "\n",
    "T_normalized = (unique_samples[\"T\"] - T_mean) / T_std_val\n",
    "pH_normalized = (unique_samples[\"pH\"] - pH_mean) / pH_std_val\n",
    "\n",
    "X = np.vstack([T_normalized, pH_normalized]).T\n",
    "\n",
    "print(f\"Input matrix X shape: {X.shape}\")\n",
    "print(f\"\\nData prepared for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: Baseline Model (Constant k)\n",
    "\n",
    "## Model Specification\n",
    "\n",
    "We start with a naive baseline that assumes k is constant across all conditions:\n",
    "\n",
    "```\n",
    "k_global ~ HalfNormal(sigma=0.1)\n",
    "P(t) = S0 * (1 - exp(-k_global * t))\n",
    "sigma_obs ~ HalfNormal(sigma=0.2)\n",
    "P_obs ~ Normal(P(t), sigma_obs)\n",
    "```\n",
    "\n",
    "This model is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 5.0\n",
    "\n",
    "coords = {\n",
    "    \"samples\": unique_samples[\"SampleID\"].values,\n",
    "    \"time\": unique_times,\n",
    "}\n",
    "\n",
    "with pm.Model(coords=coords) as model_baseline:\n",
    "    time_values = pm.Data(\"time_values\", unique_times, dims=\"time\")\n",
    "    \n",
    "    k_global = pm.HalfNormal(\"k_global\", sigma=0.1)\n",
    "    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=0.2)\n",
    "    \n",
    "    P_pred = S0 * (1 - pm.math.exp(-k_global * time_values))\n",
    "    \n",
    "    P_obs = pm.Normal(\n",
    "        \"P_obs\",\n",
    "        mu=P_pred,\n",
    "        sigma=sigma_obs,\n",
    "        observed=P_obs_matrix,\n",
    "        dims=(\"samples\", \"time\")\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_baseline:\n",
    "    trace_baseline = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        random_seed=42,\n",
    "        return_inferencedata=True,\n",
    "        idata_kwargs={\"log_likelihood\": True}\n",
    "    )\n",
    "\n",
    "print(\"Baseline sampling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tasks_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: Group Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1_header",
   "metadata": {},
   "source": [
    "## TASK 1: Check Convergence for Baseline Model\n",
    "\n",
    "**Student 1**: Assess whether MCMC sampling worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_todo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print summary statistics using az.summary()\n",
    "# Check: Is R-hat < 1.01? Is ESS > 400?\n",
    "\n",
    "summary = az.summary(trace_baseline, var_names=[\"k_global\", \"sigma_obs\"])\n",
    "print(summary)\n",
    "\n",
    "# TODO: Plot trace plots using az.plot_trace()\n",
    "# Look for: \"fuzzy caterpillar\", stationarity, chain overlap\n",
    "\n",
    "# TODO: Plot posterior distributions using arviz\n",
    "# What is the posterior mean for k_global?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2_header",
   "metadata": {},
   "source": [
    "## TASK 2: Posterior Predictive Checks for Baseline\n",
    "\n",
    "**Student 2**: Generate posterior predictive samples and assess model adequacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_todo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate posterior predictive samples\n",
    "# Hint: Use pm.sample_posterior_predictive(trace_baseline)\n",
    "\n",
    "with model_baseline:\n",
    "    ppc_baseline = ...\n",
    "\n",
    "# Extract predictions\n",
    "ppc_samples = ...\n",
    "ppc_samples_flat = ppc_samples.reshape(-1, n_samples, n_timepoints) # this helps you to separate the dimensions later\n",
    "\n",
    "# Plotting code provided (feel free to use arviz instead if you prefer)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "sample_ids_plot = rng.choice(range(n_samples), size=6, replace=False)\n",
    "\n",
    "for idx, sid in enumerate(sample_ids_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    T_val = unique_samples.loc[sid, \"T\"]\n",
    "    pH_val = unique_samples.loc[sid, \"pH\"]\n",
    "    \n",
    "    ppc_sample = ppc_samples_flat[:, sid, :]\n",
    "    \n",
    "    for pct in [5, 25, 50, 75, 95]:\n",
    "        alpha = 0.3 if pct in [5, 95] else 0.5\n",
    "        ax.plot(unique_times, np.percentile(ppc_sample, pct, axis=0), \n",
    "               alpha=alpha, color=\"blue\")\n",
    "    \n",
    "    ax.scatter(unique_times, P_obs_matrix[sid, :], color=\"red\", s=30, zorder=10, label=\"Observed\")\n",
    "    \n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    ax.set_ylabel(\"Product [P]\")\n",
    "    ax.set_title(f\"Sample {sid}: T={T_val:.1f}°C, pH={pH_val:.2f}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Baseline Model: Posterior Predictive Checks\", fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Answer these questions:\n",
    "# 1. Does the model capture all the variation in the data?\n",
    "# 2. Are some samples consistently over/under-predicted?\n",
    "# 3. What does this tell you about the constant-k assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3_header",
   "metadata": {},
   "source": [
    "## TASK 3: Build the Hybrid GP Model\n",
    "\n",
    "**Student 3**: Construct a Gaussian Process model for k(T, pH).\n",
    "\n",
    "The data is prepared (X matrix with normalized T and pH). Build the GP model following this structure.\n",
    "\n",
    "Question: Why do you need a latent GP here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_todo",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"samples\": unique_samples[\"SampleID\"].values,\n",
    "    \"time\": unique_times,\n",
    "    \"features\": [\"T\", \"pH\"],\n",
    "}\n",
    "\n",
    "with pm.Model(coords=coords) as model_gp:\n",
    "    # Data containers\n",
    "    time_values = pm.Data(\"time_values\", unique_times, dims=\"time\")\n",
    "    normalised_T_pH = pm.Data(\"normalised_T_pH\", X, dims=(\"samples\", \"features\"))\n",
    "    \n",
    "    # TODO: Define GP hyperparameters\n",
    "    # Hint: Check PyMC documentation for appropriate priors\n",
    "    # Hint: You could for example use pm.HalfNormal for amplitude (eta)\n",
    "    \n",
    "    ls = ...\n",
    "    eta = ...\n",
    "    mean_log_k = pm.Normal(\"mean_log_k\", mu=-1.5, sigma=1.0) # Here I provide you with an example\n",
    "    \n",
    "    # TODO: Create RBF covariance function (see later exercises to change this)\n",
    "    # Hint: Don't forget to use input_dim=2 and ls=ls if you want to use individual length scales for T and pH\n",
    "    \n",
    "    cov_func = ...  # Your code here\n",
    "    \n",
    "    # TODO: Create Latent GP\n",
    "    # Hint: Use pm.gp.Latent with mean_func and cov_func\n",
    "    # Hint: Use pm.gp.mean.Constant(c=mean_log_k) for mean function\n",
    "    \n",
    "    gp = ...  # Your code here\n",
    "    \n",
    "    # TODO: GP prior over log(k) (I would recommend you use it this way to avoid non-positive k samples)\n",
    "    # Hint: Use gp.prior(\"log_k\",...)\n",
    "    # While GP input has two dimensions (T, pH), output (k) is one-dimensional\n",
    "    \n",
    "    log_k = ...  # Your code here\n",
    "    \n",
    "    # TODO: Transform log(k) to k (ensures k > 0)\n",
    "    # Hint: k = pm.Deterministic(\"k\", ..., dims=\"samples\")\n",
    "    \n",
    "    k = ...  # Your code here\n",
    "    \n",
    "    # TODO: Observation noise (sigma can be tuned)\n",
    "    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=0.2)\n",
    "    \n",
    "    # TODO: Mechanistic model - compute predicted P from k\n",
    "    # Hint: See yesterday's notebook and baseline model\n",
    "    # Hint: Use [:, None] and [None, :] to broadcast multiplication (match the dimensions of samples and times etc.)\n",
    "    # k has shape (n_samples,), time_values has shape (n_timepoints,)\n",
    "    # Result should have shape (n_samples, n_timepoints)\n",
    "    \n",
    "    P_pred = pm.Deterministic(\n",
    "        \"P_pred\",\n",
    "        ...,  # Your code here: S0 * (1 - pm.math.exp(...))\n",
    "        dims=(\"samples\", \"time\")\n",
    "    )\n",
    "    \n",
    "    # Likelihood\n",
    "    P_obs = pm.Normal(\n",
    "        \"P_obs\",\n",
    "        mu=P_pred,\n",
    "        sigma=sigma_obs,\n",
    "        observed=P_obs_matrix,\n",
    "        dims=(\"samples\", \"time\")\n",
    "    )\n",
    "\n",
    "# Visualize the computation graph\n",
    "pm.model_to_graphviz(model_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample from the hybrid model\n",
    "# Hint: Increase target_accept for better sampling with GPs if necessary\n",
    "# This will take longer than the baseline model\n",
    "\n",
    "with model_gp:\n",
    "    trace_gp = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        return_inferencedata=True,\n",
    "        idata_kwargs={\"log_likelihood\": True} # in case you want to do model comparison later\n",
    "    )\n",
    "\n",
    "print(\"Hybrid model sampling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4_header",
   "metadata": {},
   "source": [
    "## TASK 4: Check Convergence and Visualize k(T, pH)\n",
    "\n",
    "**Student 4**: Check diagnostics for the hybrid model and visualize the learned surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print summary statistics for GP hyperparameters\n",
    "# Check: R-hat < 1.01? ESS > 400? Any divergences?\n",
    "\n",
    "summary = az.summary(trace_gp, var_names=[\"ls\", \"eta\", \"mean_log_k\", \"sigma_obs\"])\n",
    "print(summary)\n",
    "\n",
    "divergences = trace_gp.sample_stats[\"diverging\"].sum().item()\n",
    "print(f\"\\nDivergences: {divergences}\")\n",
    "\n",
    "# TODO: Plot traces for GP hyperparameters\n",
    "\n",
    "# TODO: What do the length scales tell you?\n",
    "# How quickly does k vary with T vs pH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the learned k(T, pH) surface\n",
    "\n",
    "# Extract posterior samples of k\n",
    "k_posterior = trace_gp.posterior[\"k\"].values\n",
    "k_mean = k_posterior.mean(axis=(0, 1))\n",
    "k_std = k_posterior.std(axis=(0, 1))\n",
    "\n",
    "# Plotting code provided\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Posterior mean\n",
    "scatter1 = axes[0].tricontourf(unique_samples[\"T\"], unique_samples[\"pH\"], k_mean, \n",
    "                                levels=20, cmap=\"viridis\")\n",
    "plt.colorbar(scatter1, ax=axes[0], label=\"k (1/min)\")\n",
    "axes[0].scatter(unique_samples[\"T\"], unique_samples[\"pH\"], c=\"red\", s=20, alpha=0.3)\n",
    "axes[0].set_title(\"Learned k(T, pH) - Posterior Mean\")\n",
    "axes[0].set_xlabel(\"Temperature (°C)\")\n",
    "axes[0].set_ylabel(\"pH\")\n",
    "\n",
    "# Uncertainty\n",
    "scatter2 = axes[1].tricontourf(unique_samples[\"T\"], unique_samples[\"pH\"], k_std, \n",
    "                                levels=20, cmap=\"Reds\")\n",
    "plt.colorbar(scatter2, ax=axes[1], label=\"Std Dev\")\n",
    "axes[1].scatter(unique_samples[\"T\"], unique_samples[\"pH\"], c=\"blue\", s=20, alpha=0.3)\n",
    "axes[1].set_title(\"Posterior Uncertainty\")\n",
    "axes[1].set_xlabel(\"Temperature (°C)\")\n",
    "axes[1].set_ylabel(\"pH\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Answer these questions:\n",
    "# 1. Where does k appear highest?\n",
    "# 2. Where is uncertainty highest?\n",
    "# 3. How does uncertainty relate to data coverage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5_header",
   "metadata": {},
   "source": [
    "## TASK 5: Compare Models\n",
    "\n",
    "**Student 5**: Generate posterior predictive checks for the hybrid model and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_todo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate posterior predictive for hybrid model\n",
    "# Use the same approach as Task 2\n",
    "\n",
    "with model_gp:\n",
    "    ppc_gp = ...\n",
    "\n",
    "# Extract predictions\n",
    "ppc_gp_samples = ...\n",
    "ppc_gp_flat = ...\n",
    "\n",
    "# Side-by-side comparison (plotting code provided)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "sample_ids_plot = rng.choice(range(n_samples), size=3, replace=False)\n",
    "\n",
    "for col, sid in enumerate(sample_ids_plot):\n",
    "    T_val = unique_samples.loc[sid, \"T\"]\n",
    "    pH_val = unique_samples.loc[sid, \"pH\"]\n",
    "    \n",
    "    # Baseline (top row)\n",
    "    ax = axes[0, col]\n",
    "    ppc_sample = ppc_samples_flat[:, sid, :]\n",
    "    \n",
    "    for pct in [5, 50, 95]:\n",
    "        alpha = 0.3 if pct in [5, 95] else 0.8\n",
    "        ax.plot(unique_times, np.percentile(ppc_sample, pct, axis=0), \n",
    "               alpha=alpha, color=\"blue\")\n",
    "    ax.scatter(unique_times, P_obs_matrix[sid, :], color=\"red\", s=30, zorder=10)\n",
    "    ax.set_title(f\"Baseline: T={T_val:.1f}°C, pH={pH_val:.2f}\")\n",
    "    ax.set_ylabel(\"Product [P]\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hybrid (bottom row)\n",
    "    ax = axes[1, col]\n",
    "    ppc_sample = ppc_gp_flat[:, sid, :]\n",
    "    \n",
    "    for pct in [5, 50, 95]:\n",
    "        alpha = 0.3 if pct in [5, 95] else 0.8\n",
    "        ax.plot(unique_times, np.percentile(ppc_sample, pct, axis=0), \n",
    "               alpha=alpha, color=\"green\")\n",
    "    ax.scatter(unique_times, P_obs_matrix[sid, :], color=\"red\", s=30, zorder=10)\n",
    "    ax.set_title(f\"Hybrid: T={T_val:.1f}°C, pH={pH_val:.2f}\")\n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    ax.set_ylabel(\"Product [P]\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Answer these questions:\n",
    "# 1. Which model better captures individual curves?\n",
    "# 2. How do the uncertainty bands differ?\n",
    "# 3. Where does each model still struggle?\n",
    "# 4. Can you think of other ways to compare the models than just visually?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6_header",
   "metadata": {},
   "source": [
    "## TASK 6: Experiment with Kernels and Length Scales\n",
    "\n",
    "**Student 6**: Explore different kernel choices and discuss length scale interpretation.\n",
    "\n",
    "### Part A: Try another kernel, e.g. Matern family (presented by Austin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_matern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the GP model from Task 3 and replace ExpQuad with another\n",
    "# Hint: pm.gp.cov.Matern52(input_dim=2, ls=ls) is the general syntax\n",
    "\n",
    "# Build model, sample, and compare learned surfaces\n",
    "# Do the surfaces look similar or different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6_discussion",
   "metadata": {},
   "source": [
    "### Part B: Discussion Questions\n",
    "\n",
    "**About Kernels:**\n",
    "\n",
    "1. What is the difference between RBF and Matern-5/2 kernels?\n",
    "2. Do the learned surfaces look similar or different? Why?\n",
    "3. When might kernel choice matter more?\n",
    "\n",
    "**About Length Scales:**\n",
    "\n",
    "4. What do the length scales (ls_T and ls_pH) represent?\n",
    "   - Small ls = ? \n",
    "   - Large ls = ?\n",
    "\n",
    "5. Should length scales be the same for T and pH?\n",
    "   - Why or why not?\n",
    "   - What do your posterior values suggest?\n",
    "\n",
    "6. What happens with very tight priors (e.g., ls ~ Gamma(0.1, 0.1))?\n",
    "\n",
    "7. What happens with very diffuse priors (e.g., ls ~ Gamma(5, 5))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Optional): Experiment with different length scale priors\n",
    "# Try: ls = pm.Gamma(\"ls\", alpha=5, beta=5, dims=\"features\")  # tight\n",
    "# Try: ls = pm.Gamma(\"ls\", alpha=1, beta=0.5, dims=\"features\")  # diffuse\n",
    "\n",
    "# How does this affect the learned surface?\n",
    "# Which prior gives better fits?\n",
    "\n",
    "\n",
    "\n",
    "# Hint: If you need to visuale different priors beforehand, you can use this code snippet:\n",
    "\n",
    "# n_samples = 5000\n",
    "\n",
    "# with pm.Model() as model:\n",
    "#     gamma1 = pm.Gamma(\"gamma1\", alpha=0.1, beta=0.1)\n",
    "#     gamma2 = pm.Gamma(\"gamma2\", alpha=5, beta=5)\n",
    "    \n",
    "#     prior_samples = pm.sample_prior_predictive(samples=n_samples)\n",
    "\n",
    "# # Extract samples\n",
    "# g1 = prior_samples.prior[\"gamma1\"].values.flatten()\n",
    "# g2 = prior_samples.prior[\"gamma2\"].values.flatten()\n",
    "\n",
    "# # Plot\n",
    "# plt.figure()\n",
    "# plt.hist(g1, bins=100, density=True, alpha=0.5, label=\"alpha=0.1, beta=0.1\")\n",
    "# plt.hist(g2, bins=100, density=True, alpha=0.5, label=\"alpha=5, beta=5\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"x\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.title(\"Different Gamma Priors\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 5: Bonus Questions\n",
    "\n",
    "If you finish early, discuss these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus_questions",
   "metadata": {},
   "source": [
    "## Bonus 1: Optimal Conditions\n",
    "\n",
    "According to your GP model, what are the optimal T and pH?\n",
    "\n",
    "```python\n",
    "idx_max = k_mean.argmax()\n",
    "T_optimal = unique_samples.loc[idx_max, \"T\"]\n",
    "pH_optimal = unique_samples.loc[idx_max, \"pH\"]\n",
    "```\n",
    "\n",
    "How confident is the model at this optimum?\n",
    "\n",
    "## Bonus 2: Experimental Design\n",
    "\n",
    "If you could collect 5 more samples, where would you place them?\n",
    "- Strategy 1: High uncertainty regions\n",
    "- Strategy 2: Around the optimum\n",
    "\n",
    "Which strategy would you choose and why?\n",
    "\n",
    "## Bonus 3: Model Assumptions\n",
    "\n",
    "What assumptions does the hybrid model make?\n",
    "1. First-order kinetics\n",
    "2. k varies smoothly with T and pH\n",
    "3. Independent observations\n",
    "4. Constant observation noise\n",
    "\n",
    "Which might be violated in real data?\n",
    "\n",
    "## Bonus 4: Extensions\n",
    "\n",
    "How would you extend this to 3+ factors (e.g., T, pH, substrate concentration)?\n",
    "\n",
    "What about alternative mechanistic models (e.g., Michaelis-Menten) or fully mechanistic models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **MCMC Diagnostics**: $\\hat{R}, ESS, and traces tell you if sampling worked\n",
    "2. **Posterior Predictive Checks**: Tell you if the model is adequate\n",
    "3. **Good diagnostics ≠ good model**: Convergence doesn't guarantee correctness\n",
    "4. **Hybrid Models**: Combine mechanistic knowledge with data-driven flexibility\n",
    "5. **GPs**: Model unknown functions and provide uncertainty\n",
    "6. **Kernels**: Encode smoothness assumptions\n",
    "7. **Length scales**: Tell you how quickly functions vary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-teaching311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
